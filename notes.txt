Notes on Linux:
# LINUX COMMANDS:

- tail: read the content of a file frm the last bottom. i.e tail -3 txt - reads the last 3 lines
- head: read the content of a file from the upper top. i.e head -3 txt - reads the first 3 lines 
- which: reveals the location of a command is stored (locate a command)
- info: reveals the information of a command
- cd: to change directory; cp or cp ~ - to the home directory; cp / - to the root directory
- mkdir: to create a new directory
- man:  reveal information of a command. Quite similar to the info command
- pwd: this command prints the working directory
- rm: remove a directory. using "rm --help or man rm" will bring several flags for the command
- cp: copy from a directory to another
- mv: mv a file from a directory to another; it also renames a file
- echo: output a content
- ls: a command for listing contents of a directory. using ls -r - reverse the order (sorting) of the content; using ls *.txt - will list every files ending with .txt extension; ls -d */ - will list only directories
- you can use the Ctrl + R + "search term" to find quickly find previously used command
- df: The df command stands for "disk-free" and shows available and used disk space on the linux system. df -h shows the disk space info in human readable format.
- free: the free shows the available  memory ram information on the system. free -h also shows the info in human readable form.

#TREE STRUCTURE ON LINUX

- mkdir with flag (-p) will create several nested directories.
	using tree command (after installing) will show the tree structure of a directory
	
	"sudo apt install tree" - will install tree
	
	tree -a (list hidden files in the tree)
	tree -d (list only directories in the tree)
	......'
- bin => usr/bin - directory referring to binary files in the linux
- lib => usr/lib / usr/lib32/ usr/lib64/ usr/libx32 - are referred to as libraries that help to runs specific programs.
- lost+found: It's a special directory that contains data that has become obsolete. Can be used to recover lost files.
- opt: a directory showing your custom application. Example, you'll find a VBox application in this folder
- proc: The /proc directory is a virtual filesystem that provides information about the system's processes and kernel.
- root: The root directory (/) is the mother of all files and directories of the Linux system. 
- boot: The boot directory hosts all the boot files for your linux operating system. It's  a don'touch directory except you know what you're doing.
- dev: Your device directory
- etc: The directory holding configuration files
- run: The run directory is the mount point for tempfs filesystem in the computer memory. Temporary data used by memory are kept there.
- sbin: Sbin directory stores the binaries required by the operating system for system management.
- snap: The /snap directory is, by default, where the files and folders from installed snap packages appear on your system. i.e. snap install tree
- swapfile: the directory holds: swap space in Linux is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space.
- srv: The term srv stands for service. The /srv directory contains site-specific data for your Linux distribution.
- var: /var is a standard directory that stands for "variable files.
- sys: /sys/class` is a directory in the Linux filesystem that provides a way to interact with the kernel and access information about various classes of devices and subsystems.
- usr: A very important directory that comprises libraries, binaries, and documentation for installed software applications. 

# OTHER COMMANDS TO USE IN THE LINUX FILE SYSTEM
- wget/curl: Wget is a command used to download files. It retrieves files using HTTP, HTTPS, and FTP protocols and is useful for downloads in unstable networks. use curl url --output "filename.ext"
- diff: diff command shows the difference between two files.
- vim/nano: the command helps to open and edit files using vim / nano respectively
- useradd: A command used to add a user 
- adduser: Used to create a new user. It also prompts you to enter password and other details for the new user immediately, unlike the useradd command.
- passwd: A command used to set password to a user
- userdel / deluser: Delete a user account and related files. sudo deluser --remove-all-files user_name 
- grep: Grep command is used to return. i.e. cat mytext.txt | grep es - will return contents with es; ls grep 01 will return files or directory with 01
- history: Bring back all the commands you've been using. You can also use a reverse search by typing Ctrl + R


# LINUX INODE (/df -i | /ls -i | /stat | /chmod | /In)
- stat: running: stat filename will reveal the metadata of the file


# LINUX - FILE PERMISSIONS

File Permissions and Access Rights
Understanding how to manage file permissions and ownership is crucial in Linux. This knowledge empowers you to control access to files and directories, ensuring the security and
integrity of your systenm Let's explore some essential commands and concepts related to file permissions and ownership.
In Linux, managing file permissions and ownership is vital for controlling who can access, modify, or execute files and directories. Understanding these concepts allows you to maintain
the security and integrity of your system, Let's delve into the key commands and concepts related to file permissions and ownership.
Numeric Representation of Permissions
In Linux, permissions are represented using numeric values. Each permission (no permission, read, write, and execute) is assigned a numeric value:
no permissions = O
read = 4
write = 2, and

These values are combined to represent the permissions for each user class. Lets consider a few examples.
Permissions Represented by 7
(read) + 2 (write) + 1 (execute) = 7
Symbolic: rwx
Meaning: Read, write, and execute permissions are all granted.
Example Context: A script file that the owner needs to read, modify, and execute.
Permissions Represented by 5
(read) + 1 (execute) = 5
Symbolic: r-x
Meaning: Read and execute permissions are granted, but write permission is not.
Example Context: A shared library or a command tool that users can execute and read but not modify.
Permissions Represented by 6
(read) + 2 (write) = 6
Symbolic: rw-
Meaning: Read and write permissions are granted, but execute permission is not.
Example Context: A document or a configuration file that the owner needs to read and modify but not execute.


Shorthand Representation of Permissions* *
In addition to the numeric way of showing permissions, Linux also has a shorthand, or symbolic, method for representing file permissions.
Understanding User Classes from a Permissions Perspective
Before diving into shorthand permissions, it's important to understand the concept of "user classes" in the context of Linux permissions. Think of user classes as categories of users that
Linux recognizes when deciding who can do what with a file. There are three main classes:
Owner: The person who created the file. Often referred to as 'user'.
Group: A collection of users who share certain permissions for the file.
Others: Anyone else who has access to the computer but doesn't fall into the first two categories.
The Role of Hyphens (-) in Permission Representation
When discussing permissions, you might notice hyphens (-) being mentioned. In the context of Linux file permissions, a hyphen doesn't actually represent a user class. Instead, it's
used in the symbolic representation of permissions to show the absence of a permission.
Lets get a bit practical with examples. Get onto your Linux terminal and run Is -latr

❯ ls -latr
total 24
-rw-r--r--   1 dare  staff  6332 Jan 29 22:44 README.md
drwxr-xr-x   7 dare  staff   224 Feb  4 11:56 Hands-On-Projects
drwxr-xr-x   8 dare  staff   256 Feb  4 11:56 .
-rwxr-xr-x   1 dare  staff   133 Feb  4 11:56 sync_img_to_s3.sh
drwxr-xr-x  37 dare  staff  1184 Feb  4 12:52 image
drwxr-xr-x   8 dare  staff   256 Feb  4 13:52 Quizzes
drwxr-xr-x   5 dare  staff   160 Feb  5 09:28 ..
drwxr-xr-x  15 dare  staff   480 Feb  6 21:34 .git


Let's break it down to understand what each part means:
In the output above, you will notice that some of the first character can be a - or d d means it's a directory, - means it's a file.
The next three characters (rwx) show the permissions for the owner. r stands for read, w for write, and x for execute.
If a permission is not granted, you'll see a - in its place (e.g., r-x means read and execute permissions are granted, but write permission is not).

The hyphen separates, owner, group, and others
The following three characters after the owner's permissions represent the group's permissions, using the same r, w, and x notation.
The last three characters show the permissions for others.
The order the user class is represented is as follow;
The first hyphen " "
- is the user
The second hyphen "-" is the group
The third hyphen "-" is others


# LINUX TERMINAL FILE DESCRIPTORS AND REDIRECTIONS
A file descriptor (FD, less frequently fildes) is a process-unique identifier (handle) for a file or other input/output resource, such as a pipe or network socket. Here are the three file descriptors:
	1. Standard In		stdin	0	Input from the keyboard
	2. Standard Out		stdout	1	Output to the console
	3. Standard Error	stderr	2	Error output to the console
	
Redirections: Redirection allows commands' file handles to be duplicated, opened, closed, made to refer to different files, and can change the files the command reads from and writes to. Redirection may also be used to modify file handles in the current shell execution environment.
	"find ./ -name newfile 2> stderr.txt" - It will redirect the error (stderr (2) - one of the 3 file descriptor) into the stderr.txt file and print to console the file path of the newfile.

- ps: The Process Status (ps) command in Linux is a powerful tool that allows you to view information about the processes running on your Linux system. We can also use the top or htop (after installing) to check all the processes running on the linux system.
	The ps -p <PID> command is pretty straightforward to get the process information of a PID. 
	The pwdx <PID> command will reveal the process file path
	
- lsof: lsof command stands for "List Open Files". This command provides a list of files that are opened. Basically, it gives the information to find out the files which are opened by which process. 	


# LINUX TERMINAL - USER GROUPS AND PERMISSIONS
- groupadd: command let you create a group which you can add any user into
	groupadd "group_name"
- groupdel: command to delete a groupd
	groupdel "group_name"
	
- usermod: the command let you modify the details of a user. I.e, 
	usermod -aG "group_name" "user" - will add the user to the group without removing the user from any existing group (because of the -G flag)



Some QUESTIONS/ANSWERS:

What command installs software on a Red Hat/Fedora-based system?
Ans => dnf install
 
What command is used to update the package lists on a Debian-based Linux system?
Ans => apt update 

How would you uninstall a software package on a Red Hat/Fedora-based system using the package manager?
Ans => yum remove package-name
 
Which command is used to clean the package cache on a Debian/Ubuntu system?
Ans => clean apt-cache

To upgrade all installed packages on a Debian-based system, which command would you use?
Ans => apt-get dist-upgrade


To list files in a directory, which command would you use?
Ans => ls

How do you navigate to the home directory of the current user?
Ans => cd ~ or cd

Which command is used to remove a file in Linux?
Ans => rm

What command is used to create a new directory?
Ans => mkdir

How do you display the current working directory in the Linux terminal?
Ans => pwd


To remove read and write permissions for the group and others on a file, what command would you use?
Ans => chmod go-rw file
	g - group
	o - others

What command is used to change the ownership of a file in Linux?
Ans => chown

How would you give read, write, and execute permissions to the owner of a file?
chmod u+rwx file
	owner is the user of the file = u

What command is used to set the sticky bit on a directory?
Ans => chmod 1777

Which command is used to change the group ownership of a file?
Ans =>chgrp


# ADVANCED FILE OPERATIONS
ARCHIVING AND COMPRESSING OF FILES
POPULAR METHODS OF ARCHIVING AND COMPRESSING:
- ZIP: The fact that it is cross-platform accessible gives this method an advantage over others. You can access and open a zip file regardless of the operating system because Linux, Windows®, and MacOS® all support zip files by default.
	$ zip -r <name of the zip file>.zip <directory or file(s) you want to compress>
	To archive a single, we can use the following command:
	$ zip example.zip examplefile.txt
	
	To add multiples or directory to the zip archive, we use:
	$ zip -r example.zip /home/users/Pictures - -r is to add all the files in the directory recursively
	
	If you received a zipped file, use the unzip command to unzip it:
	$ unzip examples.zip
	
	What if you want to unzip the file into your Pictures directory, and you are currently in a different directory?
	$ unzip -r examples.zip -d /home/user/Pictures
	Otherwise, the files will be extracted in the zipped directory

- TAR: The tar command has a few more options than the zip command did. The most commonly used options for the tar command
includes the following:

    -c: creates a new .tar archive file
    -v: verbosely shows the tar process so you can see all the steps in the process
    -f: specifies the file name type of the archive file
    -x: extracts files from an existing .tar file

The following example shows the basic syntax of the tar command to create an archive:
	$ tar -cvf examples.tar /home/user/Pictures
	$ tar -cvf examples.tar ‘*.jpg’ - will add all the .jpg files in the archive

To extract files from an archive in the same directory, use:
	$ tar -xvf examples.tar /home/user/Pictures

To extract files from an archive to a different directory, use:
	$ tar -xvf examples.tar -C /path/to/desired/directory/location/
	
- TAR.GZ: Tar.gz files add compression to the archive function of the tar command by using the gzip function.
	$ tar -zcvf <archive name>.tar.gz /directory/you/want/to/compress
	OR
	$ tar -zcvf <archive name>.tar.gz ‘*.jpg’
 


# SHELL SCRIPTING
- Shell Scripting: Is the process of writing and executing a series of instructions in a shell to automate tasks. A shell script is essentially a script or program written in a shell language, such as Bash, sh, zsh, or PowerShell.

- The bash Shell Program: The command-line interface you type into is a Bash Shell. It's specificallt a running instance of the program found at /bin/bash. Bash is an sh-compatible command language interpreter.

	A simple hello-world script:
	
	#! /bin/bash
	target="World"
	echo "Hello, ${target}"
	
	Bash Script can have comments. Using the hash (#) symbol before a code line will comment out the code. For example:
	
	# echo "Hello World!" - this line of code won't run because we've commented out the code using the # symbol.


# HANDS-ON: VERSION CONTROL SYSTEM AND WHY IT IS NEEDED

A Version Control System (VCS) is a vital tool in software development, designed to track and manage changes to code or documents over time. It enables multiple developers to collaborate on the same project efficiently, by controlling and merging changes made by different team members. 
Imagine you're working in a team and your project is about creating a website for a an Al startup company. The website includes various sections like Home, About Us, Services, and Contact Information, Each member of your team is responsible for a different section of the site. Without a Version Control System (VCS), managing this collaboration efficiently would be challenging.

Lets take a view of an example of working without a Version Control System. 

Overwriting Work
If a member "Tom" makes changes to the home page file "index.html" to update the navigation and at the same time, another team member "Jerry" makes changes to add contact information to the footer of the same home page thereby editing the same "index.html" file, Without VCSi the last person to upload their version of the file to the shared folder or server would overwrite the other person's changes, resulting in lost work.

How VCS Solves These Problems
Concurrent Development: With a VCS, each team member can work on their sections simultaneously without fear of overwriting each other's work. The VCS tracks all changes and manages different versions of the files, allowing changes to be merged together eventually. Lets go through an example together to simulate this experience using a VCS tool, "Git"

Introducing Git: A Leading Version Control System
Git is a tool that helps people work together on computer projects, like building a website. Think of it as a shared folder on your computer, but much smarter. It keeps track of all the changes everyone makes, so if something goes wrong, you can always go back to a version that worked. It also lets everyone work on their parts at the same time without getting in each other's way.
When working on a project, especially with a team, it's easy for things to get mixed up if you're not careful. For example, if two people try to change the same thing at the same time, it could cause problems. 

Git helps prevent these kinds of mix-ups.

Conceptualising Git Set Up with Tom and Jerry

1. Initial Setup:
Both Tom and Jerry have Git installed on their computers. 
They clone (or download) the project repository from a central repository (like GitHub, GitLab, or Bitbucket) to their local machines. This gives them each a complete copy of the project, including all its files and version history.

2. Tom and Jerry Start Working:
Tom and Jerry pull the latest changes from the central repository to ensure they start with the most current version of the index.html file. 
They both create a new branch from the main project. A branch in Git allows developers to work on a copy of the codebase without affecting the main line of development. Tom names his branch update-navigation, and Jerry names his add-contact-info.

3. Making Changes:
On his branch, Tom updates the navigation bar in index.html.
Simultaneously, Jerry works on his branch to add contact information to the footer of the same file.
They commit their changes to their respective branches. A commit in Git is like saving your work with a note about what you've done.

Merging Changes:
Once they're done, Tom and Jerry push their branches to the central repository. 
Tom decides to merge his changes first. He creates a pull request (PR) for his branch update-navigation. A PR is a way to tell the team that he's done and his code is ready to be reviewed and merged into the main project.
After reviewing Tom's changes, the team merges his PR into the main branch, updating the index.html file on the main project line.
Jerry then updates his branch with the latest changes from the main project to include Tom's updates. This step is crucial to ensure that Jerry is working with and integrating his changes into the most current version of the project.
Jerry resolves any conflicts that arise from Tom's changes and his own. Git provides tools and commands to help identify and resolve these conflicts.
Jerry then pushes his updated branch and creates a PR for his changes. The team reviews Jerry's additions, and once they're approved, his changes are merged into the main project.

# GIT BRANCHING AND MERGING

Part 3: Merging Changes 
After both Tom and Jerry have pushed their changes, you (or another team member) can review and merge these changes into the main project. The process involves
1. Creating a Pull Request
2. Merging the Pull Request into the main branch.

Understanding Pull Requests:
A Pull Request (PR) is a feature used in GitHub (and other Gitbased version control systems) that allows you to notify team members about the changes you've pushed to a branch in a repository. Essentially, it's a request to review and pull in your contribution to the main project. Pull requests are central to the collaborative development process, enabling team members to discuss, review, and make further changes before changes are merged.

How to Create a Pull Request on GitHub

After both Tom and Jerry have pushed their work to their respective branches, the next step is to create a pull request for each of them. Here's how Tom would create a pull request for his changes:
1. Navigate to Your GitHub Repository: Open your web browser and go to the GitHub page for the repository.
2. Switch to the Branch: Click on the branch dropdown menu near the top left corner of the file list and select the branch Tom have been working on, in this case, update-navigation branch.
3. Create New Pull Request: Click the "New pull request" button next to the branch dropdown menu.


GitHub will take you to a new page to initiate a pull request. It automatically selects the main project's branch as the base and your recently pushed branch as the compare branch.
4. Review Tom's Changes: Before creating the pull request, Tom would review his changes to ensure everything is correct. GitHub shows the differences between the base branch and Tom's branch. It's a good opportunity for Tom to double-check his work.

5. Create the Pull Request: If everything looks good, click the "Create pull request" button. Provide a title and description for the pull request. The title should be concise and descriptive, and the description should explain the change that the pull request is about, why it's needed, and any other relevant details.
After filling in the information, click 'Create pull request" again to officially open the pull request.

Reviewing and Merging Tom's Pull Request 
Once the pull request is created, it becomes visible to other team members who can review the changes, leave comments, and request additional modifications if necessary (This is an example of what collaboration is about in DevOps). When the team agrees that the changes are ready and good to go, someone with merge permissions can merge the pull request, incorporating the changes from Tom's update-navigation branch into the main branch.

Following the same process, Jerry would create a pull request for his add-contact-info branch after Tom's changes have been merged, ensuring that the project stays up to date and conflicts are minimized Updating Jerry's Branch with Latest Changes
Before Jerry merges his changes into the main branch, it's essential to ensure his branch is up-to-date with the main branch. This is because other changes (like Tom's updates) might have been merged into the
main branch after Jerry started working on his feature. Updating ensures compatibility and reduces the chances of conflicts.

Steps to Update Jerry's Branch:
On the terminal, Switch to Jerry's Branch:
	git checkout add-contact-info

Pull the Latest Changes from the Main Branch:
	git pull origin main

Purpose: This command fetches the changes from the main branch (Remember, main branch now has Tom's changes) and merges them into Jerry's add-contact-info branch. It ensures that any updates made to the main branch, like Tom's merged changes, are now included in Jerrys branch. This step is crucial for avoiding conflicts and ensuring that Jerry's work can smoothly integrate with the main project.
Merge the pull request to the main branch: Click the "Merge pull request" button to merge Tom's changes into the main branch. This action combines Tom's contributions with the rest of the project, completing the collaborative workflow.

Finalizing Jerry's Contribution
Assuming there are no conflicts, Jerry's branch is now ready to be merged back into the main project.

Push the Updated Branch to GitHub:
	git push origin add-contact-info
This command uploads Jerry's changes to GitHub. Now, his branch reflects both his work and the latest updates from the main branch.

The origin keyword in the command refers to the default name Git gives to the remote repository from which you cloned your project. It's like a shortcut or an alias for the full URL of the repository in GitHub.

Create the Pull Request (PR) for Jerry's changes, similar to how you did for Tom.
Merge Jerry's Pull Request. Complete the process by merging the PR into the main branch.

This simulated workflow illustrates how Git facilitates collaborative development, allowing multiple developers to work simultaneously on different aspects of a project and merge their contributions seamlessly, even when working on the same files.


Jamie's Work: Updating Events Page
Repeat the same flow for Jamie's work on Events Page. Ensure Jamie's work is in update-events branch.
Pull the latest changes from the main branch into update-events before raising the PR.


ENVIRONMENT VARIABLES 7 INFRASTRUCTURE ENVIRONMENTS:

Understanding Environment Variables & Infrastructure Environments: Key Differences
As we delve deeper into the world of technology and its building blocks, two essential concepts often come to the forefront: "Infrastructure Environments" and "Environment Variables."

Despite both terms featuring "Environment," they play distinct roles in the realm of scripting and software development. This common terminology can lead to confusion, making it crucial to distinguish and understand each concept from the outset. Infrastructure Environments Infrastructure environments refer to the various settings where software applications are developed. tested, and deployed, each serving a unique purpose in the software lifecycle. 

Lets say you are working with a development team to build a FinTech product. They have 2 different AWS accounts. The journey would be something like;
1. VirtualBox + Ubuntu: The development environment where all local development is done on your laptop 
2. AWS Account 1: The testing environment where, after local development is completed, the code is pushed to an EC2 instance here for further testing
3. AWS Account 2: The production environment, where after tests are completed in AWS Account 1:, the code is pushed to an EC2 instance in AWS Account 2, where the customers consume the Fintech product through a website.

On the other hand, environment variables are key-value pairs used in scripts or computer code to manage configuration values and control software behavior dynamically.

Environment Variables:
Imagine your FinTech product needs to connect to a database to fetch financial data. However, the details of this database connection, like the database URL, username, and password differ between your development, testing, and production environments. 
If you need to develop a shell script that will be reused across all the 3 different environments, then it is important to dynamically fetch the correct value for your connectivity to those environments.

Here's how environment variables come into play: 

Development Environment (VirtualBox + Ubuntu):
Environment Variables:
DB URL-localhost
DB USER=test user
DB_PASS=test_pass

Here, the environment variables point to a local database on your laptop where you can safely experiment without affecting real or test data.

Testing Environment (AWS Account 1):
Environment Variables:

DB_URL=testing-db.example.com
DB_USER=testing_user
DB_PASS=testing_pass

In this environment, the variables are configured to connect to a remote database dedicated to testing. This ensures that tests are performed in a controlled environment that simulates production settings without risking actual customer data.

Production Environment (AWS Account 2):
Environment Variables:

DB_URL=production-db.example.com
DB_USER=prod_user
DB_PASS=prod_pass

Finally, when the application is running in the production environment. The environment variables switch to ensure the application connects to the live database. This is where real customer interactions happen, and the data needs to be accurate and secure.
By clarifying these differences early on, we set a solid foundation for navigating the complexities of technology development with greater ease and precision. 

Now lets begin developing our shell script to manage cloud infrastructure.

aws_cloud_manager.sh script:
By the end of this mini project, we would have started working on the aws_cloud_manager.sh script where environment variables will be defined, and command line arguments are added to control if the script should run for local environment, testing or production environment.

Developing a shell script is usually done by starting with incremental changes. 
Lets begin by creating environment variable to determine if the script is running for local, testing, or production environment. 
If you are on Mac, open up your Mac Terminal 
If you are on Windows, login to your Ubuntu desktop in virtual box and open up the terminal.
If you don't use Virtual box, spin up an EC2 instance and name it local so it represents your local environment, then login to it.
Create a shell script with the name aws_cloud_manager.sh
Put the code below into the file:

#!/bin/bash

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
echo "Running script for Local Environment..."
# Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
echo "Running script for Testing Environment..."
# Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
echo "Running script for Production Environment..."
# Commands for production environment
else
echo "No environment specified or recognized."
exit 2
fi

Give it the regular permission to execute on your local machine:
sudo chmod +x aws_cloud_manager.sh

If you execute this as it is, the execution should go into the else block just because there is no $ENVIRONMENT variable set. 

what if you type this on your terminal:
export ENVIRONMENT=production

Then run the script again, the output this time will be:

Running script for Production Environment...

Now, you can see how environment variables can be used to dynamically apply logic in the script based on the requirement you are trying to satisfy.
The export command is used to set key and values for environment variables.

You can also set the variable directly within the script. For example if the script was like this:

#!/bin/bash

# Initialize environment variable
ENVIRONMENT="testing"

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
  # Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
  # Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
  # Commands for production environment
else
  echo "No environment specified or recognized."
  exit 2
fi

Running this version of the script would mean every time you run it. It will consider the logic for testing environment. Because the value has been "hard coded" in the script, and that is no longer dynamic.
The best way to do this would be to use command line arguments.

Positional parameters in Shell Scripting:
As we've learned, hard-coding values directly into scripts is considered poor practice. Instead, we aim for flexibility by allowing scripts to accept input dynamically, This is where positional parameters come in  a capability in shell scripting that enables passing arguments to scripts at runtime, and then replaces the argument with the parameter inside the script, the argument passed to the script is the value that is provided at runtime. 

As in the case of the below where the argument is "testing", and it is also the value to the variable within the script.

./aws_cloud_manager.sh testing

Inside the script, we'll have:

ENVIRONMENT=$1

`$1` is the positional parameter which will be replaced by the argument passed to the script. Because it is possible to pass multiple parameters to a script, dollar sign is used to prefix the position of the argument passed to the script. Imagine if another variable within the script is called NUMBER OF INSTANCES that determines how many EC2 instances get provisioned, then calling the script might look like;

./aws_cloud_manager.sh testing 5

The positional parameter inside the script will then look like:

ENVIRONMENT=$1
NUMBER_OF_INSTANCES=$2

Each positional parameter within the script corresponds to a specific argument passed to the script, and each parameter has a position represented by an index number.
In the case of:

./aws_cloud_manager.sh testing 5

We have two positional parameters, namely position 1( testing) and position 2 (5)


Creating shell scripts to meet specific requirements is one aspect of development, but ensuring their cleanliness and freedom from bugs is equally crucial. Integrating logical checks periodically to validate data is considered a best practice in script development.
A prime example of this is verifying the number of arguments passed to the script, ensuring that the script receives the correct input required for its execution, and providing clear guidance to users in case of incorrect usage.

Below code ensures that when the script is executed, exactly 1 argument is passed to it, otherwise it fails with an exit code of 1 and an shows a message telling the user how to use the script.

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

"$#" is a The special variable that holds the number of arguments passed to the script.
"-ne" means "Not equal"
"$0" represent the positional parameter of O, which is the script itself.
Hence, if number of arguments is not equal to then show the echo message.
An updated script would look like this;

#!/bin/bash

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

# Accessing the first argument
ENVIRONMENT=$1

# Acting based on the argument value
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
else
  echo "Invalid environment specified. Please use 'local', 'testing', or 'production'."
  exit 2
fi


ENVIRONMENT VARIABLES 7 INFRASTRUCTURE ENVIRONMENTS:

Understanding Environment Variables & Infrastructure Environments: Key Differences
As we delve deeper into the world of technology and its building blocks, two essential concepts often come to the forefront: "Infrastructure Environments" and "Environment Variables."

Despite both terms featuring "Environment," they play distinct roles in the realm of scripting and software development. This common terminology can lead to confusion, making it crucial to distinguish and understand each concept from the outset. Infrastructure Environments Infrastructure environments refer to the various settings where software applications are developed. tested, and deployed, each serving a unique purpose in the software lifecycle. 

Lets say you are working with a development team to build a FinTech product. They have 2 different AWS accounts. The journey would be something like;
1. VirtualBox + Ubuntu: The development environment where all local development is done on your laptop 
2. AWS Account 1: The testing environment where, after local development is completed, the code is pushed to an EC2 instance here for further testing
3. AWS Account 2: The production environment, where after tests are completed in AWS Account 1:, the code is pushed to an EC2 instance in AWS Account 2, where the customers consume the Fintech product through a website.

On the other hand, environment variables are key-value pairs used in scripts or computer code to manage configuration values and control software behavior dynamically.

Environment Variables:
Imagine your FinTech product needs to connect to a database to fetch financial data. However, the details of this database connection, like the database URL, username, and password differ between your development, testing, and production environments. 
If you need to develop a shell script that will be reused across all the 3 different environments, then it is important to dynamically fetch the correct value for your connectivity to those environments.

Here's how environment variables come into play: 

Development Environment (VirtualBox + Ubuntu):
Environment Variables:
DB URL-localhost
DB USER=test user
DB_PASS=test_pass

Here, the environment variables point to a local database on your laptop where you can safely experiment without affecting real or test data.

Testing Environment (AWS Account 1):
Environment Variables:

DB_URL=testing-db.example.com
DB_USER=testing_user
DB_PASS=testing_pass

In this environment, the variables are configured to connect to a remote database dedicated to testing. This ensures that tests are performed in a controlled environment that simulates production settings without risking actual customer data.

Production Environment (AWS Account 2):
Environment Variables:

DB_URL=production-db.example.com
DB_USER=prod_user
DB_PASS=prod_pass

Finally, when the application is running in the production environment. The environment variables switch to ensure the application connects to the live database. This is where real customer interactions happen, and the data needs to be accurate and secure.
By clarifying these differences early on, we set a solid foundation for navigating the complexities of technology development with greater ease and precision. 

Now lets begin developing our shell script to manage cloud infrastructure.

aws_cloud_manager.sh script:
By the end of this mini project, we would have started working on the aws_cloud_manager.sh script where environment variables will be defined, and command line arguments are added to control if the script should run for local environment, testing or production environment.

Developing a shell script is usually done by starting with incremental changes. 
Lets begin by creating environment variable to determine if the script is running for local, testing, or production environment. 
If you are on Mac, open up your Mac Terminal 
If you are on Windows, login to your Ubuntu desktop in virtual box and open up the terminal.
If you don't use Virtual box, spin up an EC2 instance and name it local so it represents your local environment, then login to it.
Create a shell script with the name aws_cloud_manager.sh
Put the code below into the file:

#!/bin/bash

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
echo "Running script for Local Environment..."
# Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
echo "Running script for Testing Environment..."
# Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
echo "Running script for Production Environment..."
# Commands for production environment
else
echo "No environment specified or recognized."
exit 2
fi

Give it the regular permission to execute on your local machine:
sudo chmod +x aws_cloud_manager.sh

If you execute this as it is, the execution should go into the else block just because there is no $ENVIRONMENT variable set. 

what if you type this on your terminal:
export ENVIRONMENT=production

Then run the script again, the output this time will be:

Running script for Production Environment...

Now, you can see how environment variables can be used to dynamically apply logic in the script based on the requirement you are trying to satisfy.
The export command is used to set key and values for environment variables.

You can also set the variable directly within the script. For example if the script was like this:

#!/bin/bash

# Initialize environment variable
ENVIRONMENT="testing"

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
  # Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
  # Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
  # Commands for production environment
else
  echo "No environment specified or recognized."
  exit 2
fi

Running this version of the script would mean every time you run it. It will consider the logic for testing environment. Because the value has been "hard coded" in the script, and that is no longer dynamic.
The best way to do this would be to use command line arguments.

Positional parameters in Shell Scripting:
As we've learned, hard-coding values directly into scripts is considered poor practice. Instead, we aim for flexibility by allowing scripts to accept input dynamically, This is where positional parameters come in  a capability in shell scripting that enables passing arguments to scripts at runtime, and then replaces the argument with the parameter inside the script, the argument passed to the script is the value that is provided at runtime. 

As in the case of the below where the argument is "testing", and it is also the value to the variable within the script.

./aws_cloud_manager.sh testing

Inside the script, we'll have:

ENVIRONMENT=$1

`$1` is the positional parameter which will be replaced by the argument passed to the script. Because it is possible to pass multiple parameters to a script, dollar sign is used to prefix the position of the argument passed to the script. Imagine if another variable within the script is called NUMBER OF INSTANCES that determines how many EC2 instances get provisioned, then calling the script might look like;

./aws_cloud_manager.sh testing 5

The positional parameter inside the script will then look like:

ENVIRONMENT=$1
NUMBER_OF_INSTANCES=$2

Each positional parameter within the script corresponds to a specific argument passed to the script, and each parameter has a position represented by an index number.
In the case of:

./aws_cloud_manager.sh testing 5

We have two positional parameters, namely position 1( testing) and position 2 (5)


Creating shell scripts to meet specific requirements is one aspect of development, but ensuring their cleanliness and freedom from bugs is equally crucial. Integrating logical checks periodically to validate data is considered a best practice in script development.
A prime example of this is verifying the number of arguments passed to the script, ensuring that the script receives the correct input required for its execution, and providing clear guidance to users in case of incorrect usage.

Below code ensures that when the script is executed, exactly 1 argument is passed to it, otherwise it fails with an exit code of 1 and an shows a message telling the user how to use the script.

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

"$#" is a The special variable that holds the number of arguments passed to the script.
"-ne" means "Not equal"
"$0" represent the positional parameter of O, which is the script itself.
Hence, if number of arguments is not equal to then show the echo message.
An updated script would look like this;

#!/bin/bash

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

# Accessing the first argument
ENVIRONMENT=$1

# Acting based on the argument value
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
else
  echo "Invalid environment specified. Please use 'local', 'testing', or 'production'."
  exit 2
fi



INTRODUCTION TO DOCKER AND CONTAINERS

What are Containers?

In realm of software development and deployment, professionals used to face a dilemma. They crafted brilliant code on their local machines, only to find that when deployed to other environments, it sometimes does not work. The culprit? The notorious "it works on my machine" phenomenon.

Get started with Docker, a tool that emerged to a major problem IT industry. A man named Solomon Hykes, who, in 2013, unveiled Docker, a containerisation platform that promised to revolutionize the way IT professionals built, shipped, and ran applications. Imagine containers as magical vessels that encapsulate everything an application needs to run smoothly — its code, libraries, dependencies, and even a dash of configuration magic. These containers ensure that an application remains consistent and behaves the same, whether it's running on a developer's laptop, a testing server, or a live production environment. Docker had bestowed upon IT professionals the power to say goodbye to the days of "it works on my machine."


Advantages of Containers 
Portability Across Different Environments: In the past, deploying applications was akin to navigating a treacherous maze, with compatibility issues lurking at every turn. Docker's containers, However, encapsulate the entire application along with its dependencies and configurations. This magical package ensures that your creation dances gracefully across different platforms, sparing you from the woes of the "it works on my machine" curse. With Docker, bid farewell to the headaches of environment mismatches and embrace a world where your application reigns supreme, irrespective of its hosting kingdom. 

Resource Efficiency Compared to Virtual Machines: Docker containers share the underlying host's operating system kernel, making them lightweight and nimble. This efficiency allows you to run multiple containers on a single host without the extravagant resource demands of traditional virtual machines. Picture Docker containers as magical carriages, swiftly transporting your applications without burdening the kingdom with unnecessary excess. With Docker, revel in the harmony of resource optimization and application efficiency. 

Rapid Application Deployment and Scaling: Docker containers can be effortlessly spun up or torn down, facilitating the swift deployment of applications. Whether you're facing a sudden surge in demand or orchestrating a grand-scale production, Docker allows you to scale your applications seamlessly. Imagine commanding an army of containers to conquer the peaks of user demand, only to gracefully retreat when the storm has passed. With Docker, the ability to scale becomes a wand in your hand, transforming the challenges of deployment into a choreographed ballet of efficiency. 


Comparison of Docker Container with Virtual Machines Docker and virtual machines (VMs) are both technologies used for application deployment, but they differ in their approach to virtualization. Virtual machines emulate entire operating systems, resulting in higher resource overhead and slower performance. In contrast, Docker utilizes containerization, encapsulating applications and their dependencies while sharing the host OSs kernel. This lightweight approach reduces resource consumption, provides faster startup times, and ensures portability across different environments. 

Docker's emphasis on microservices and standardized packaging fosters scalability and efficiency, making it a preferred choice for modern, agile application development, whereas virtual machines excel in scenarios requiring stronger isolation but at the cost of increased resource usage. The choice between Docker and VMS depends on specific use cases and the desired balance between performance and isolation.


Importance of Docker
Technology and Industry Impact: The significance of Docker in the technology landscape cannot be overstated, Docker and containerization have revolutionized software development, deployment, and management. The ability to package applications and their dependencies into lightweight, portable containers addresses key challenges in software development, such as consistency across different environments and efficient resource utilization. 

Real-World Impact: Implementing Docker brings tangible benefits to organizations. It streamlines the development process, promotes collaboration between development and operations teams, and  accelerates the delivery of applications. Docker's containerization technology enhances scalability, facilitates rapid deployment, and ensures the consistency of applications across diverse environments. 

This not only saves time and resources but also contributes to a more resilient and agile software development lifecycle.

Target Audience
This course on Docker is designed for a diverse audience, including: 

DevOps Professionals: Interested in container orchestration, seeking efficient ways to manage and deploy applications, improve resource utilization, and ensure system stability. 

Developers: Who want to streamline their application development, enhance collaboration, and ensure consistency across different stages of the development lifecycle. It caters to cloud engineers, OA engineers, and other tech enthusiast who are eager to enhance their technical skills and establish a strong foundation in docker and containersation.

Professionals seeking to expand their skill set or students preparing for roles in technology-related fields will find this project beneficial. 

Getting Started With Docker

Installing Docker

We need to launch an ubuntu 20.04 LTS instance and connect to it, then follow the steps below.

Before installing Docker Engine for the first time on a new host machine, it is necessary to configure the Docker repository. Following this setup, we can proceed to install and update 
Docker directly from the repository.

Now let's first add docker's oficial GPG key.
You can learn more about GPG keys

sudo apt-get update

This a Linux command that refreshes the package list on a Debian based system, ensuring the latest software information is available for installation.

sudo apt-get install ca-certificates curl gnupg

This a Linux command that installs essential packages including certificate authorities, a data transfer tool (curl), and the GNU Privacy Guard for secure communication and package verification.

sudo install -m e755 -d /etc/apt/keyrings

The command above creates a directory (/etc/apt/keyrings) with specific permissions (0755) for storing keyring files, which are used for docker's authentication 

curl -fsSL https://download.docker.com/linux/ubuntu/gpg I sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

This downloads the Docker GPG key using curl'

sudo chmod a+r /etc/apt/keyrings/docker.gpg

Sets read permissions for all users on the Docker GPG key file within the APT keyring directory

Let's add the repository to Apt sources

echo \ "deb --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg https://download.docker.com/linux/ubuntu \) stable" I \ $(. /etc/os-release && echo "$VERSION_CODENAME" sudo tee /etc/apt/sources.list.d/docker.list > /dev/null


Install latest version of docker

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

Verify that docker has been successfully installed

sudo systemctl status docker


By default, after installing docker, it can only be run by root user or using sudo* command. To run the docker command without sudo execute the command below

sudo usermod -aG docker ubuntu

After executing the command above, we can run docker command without using superuser privileges 

Running the "Hello World" Container
Using the 'docker runs Command

The docker run* command is the entry point to execute containers in Docker. It allows you to create and start a container based on a specified Docker image. The most straightforward example is the "Hello World" container, a minimalistic container that prints a greeting message when executed.

hello-world
# Run the "Hello World" container
docker run "Hello-World" container

When you execute this command, Docker performs the following steps: 

Pulls Image (if not available locally): Docker checks if the hello-world* image is available locally. If not, it automatically pulls it from the Docker Hub, a centralized repository for

1. Docker images.
2. Creates a Container: Docker creates a container based on the hello-world' image. This container is an instance of the image, with its own isolated filesystem and runtime environment.
3. Starts the Container: The container is started, and it executes the predefined command in the image, which prints a friendly message.

Understanding the Docker Image and Container Lifecycle

Docker Image: A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Images are immutable, meaning they cannot be modified once created, Changes result in the creation of a new image. 

Container Lifecycle: Containers are running instances of Docker images. They have a lifecycle: create, start, stop, and delete*. Once a container is created from an image, it can be started, stopped, and restarted.

Verifying the Successful Execution 
You can check if the images is now in your local environment with Example Output: 

docker images

Basic Docker Commands

Docker Run 
The docker run* command is fundamental for executing containers. It creates and starts a container based on a specified image.

# Run a container based on the "nginx" image 
docker run hello-world

This example pulls the "nginx" image from Docker Hub (if not available locally) and starts a container using that image.

Docker PS
The docker command displays a list of running containers. This is useful for monitoring active containers and obtaining information such as container IDs, names, and status.

# List running containers
docker ps

To view all containers, including those that have stopped, add the -a' option:

# List all containers (running and stopped)
docker ps -a

Docker Stop
The docker stop' command halts a running containel
# Stop a running container (replace CONTAINER ID with the actual container ID)
docker stop CONTAINER ID

Docker Pull 
The docker pull& command downloads a Docker image from a registry, such as Docker Hub, to your local machine.

# Pull the latest version of the "ubuntu" image from Docker Hub
docker pull ubuntu

Docker Push 
The docker push • command uploads a local Docker image to a registry, making it available for others to pull.

# Push a local image to Docker Hub
docker push your-username/image-name

Ensure you've logged in to Docker Hub using *docker before pushing images.

Docker Images
The docker images* command lists all locally available Docker images.
# List all local Docker images
docker images

Docker RMI 
The docker rmi* command removes one or more images from the local machine.
# Remove a Docker image (replace IMAGE_ID with the actual image ID)
docker rmi IMAGE ID

These basic Docker commands provide a foundation for working with containers. Understanding how to run, list, stop, pull, push, and manage Docker images is crucial for effective containerization and orchestration. As you delve deeper into Docker, you'll discover additional commands and features that enhance your ability to develop, deploy, and maintain containerized applications.
	

Working with Docker Images

Introduction to Docker Images
Docker images are the building blocks of containers. They are lightweight, portable, and self-sufficient packages that contain everything needed to run a software application, including the code, runtime, libraries, and system tools. Images are created from a set of instructions known as a Dockerfile, which specifies the environment and configuration for the application.

Pulling Images from Docker Hub 
Docker Hub is a cloud-based registry that hosts a vast collection of Docker images. You can pull images from Docker Hub to your local machine using the *docker pull* command. To explore available images on Docker Hub, the docker command provides a search subcommand. For instance, to find the Ubuntu image, you can execute: 

docker search ubuntu

This command allows you to discover and explore various images hosted on Docker Hub by providing relevant search results. In this case, the output will be similar to this: This command allows you to discover and explore various images hosted on Docker Hub by providing relevant search results. In this case, the output will be similar to this:

To download the official Ubuntu image to your computer, use the following command: 

docker pull ubuntu

Executing this command will fetch the official Ubuntu image from Docker Hub and store it locally on your machine, making it ready for use in creating containers.

Once an image has been successfully downloaded, you can proceed to run a container using that downloaded image by employing the "run" subcommand. Similar to the hello- world example, if an image is not present locally when the docker run* subcommand is invoked, Docker will automatically download the required image before initiating the container.

To view the list of images that have been downloaded and are available on your local machine, enter the following command: 

docker images

Executing this command provides a comprehensive list of all the images stored locally, allowing you to verify the presence of the downloaded image and gather information about its size, version, and other relevant details.

As we move on in this course, you will the images to work with containers

Dockerfile
A Dockerfile is a plaintext configuration file that contains a set of instructions for building a Docker image. It serves as a blueprint for creating a reproducible and consistent environment for your application. Dockerfiles are fundamental to the containerization process, allowing you to define the steps to assemble an image that encapsulates your application and its dependencies.

Creating a Dockerfile
In this Dockerfile file, we will be using an nginx image. *Nginx' is an open source software for web serving, reverse proxying, caching, load balancing, media streaming, and more. It started out as a web server designed for maximum performance and stability.

To create a Dockerfile, use a text editor of your choice, such as vim or nano. Start by specifying a base image, defining the working directory, copying files, installing dependencies, and configuring the runtime environment.

Here's a simple example of a Dockerfile for a html file: Let's create an image with using a Dockerfile. Paste the code snippet below in a file named Dockerfile. This example assumes you have a basic HTML file named index. html' in the same directory as your Dockerfile.

# Use the official NGINX base image
FROM nginx:latest

# Set the working directory in the container
WORKDIR  /usr/share/nginx/html/

# Copy the local HTML file to the NGINX default public directory
COPY index.html /usr/share/nginx/html/

# Expose port 80 to allow external access
EXPOSE 80

# No need for CMD as NGINX image comes with a default CMD to start the server

HTML file named "index.html" in the same directory as your dockerfile

To build an image from this Dockerfile, navigate to the directory containing the file and run: 

docker build -t dockerfile .

To run a container based on the custom NGINX image we created with a dockerfile, run the command:

docker run -p 8080:80 dockerfile

Running the command above will create a container that listens on port 8080 using the nginx image you created earlier. So you need to create a new rule in security group of the EC2 instance.

Let's recall the hands-on project we did in our advanced techops curriculum. Now let's add new rules to the security group:
	On our EC2 instance, click on the security tab,
	
	Click on edit inbound rules to add new rules. This will allow incoming traffic to instance associated with the security group. Our aim is to allow incoming traffic on port 8080


Introduction to Docker Containers

Docker containers are lightweight, portable, and executable units that encapsulate an application and its dependencies. In the previous step, we worked a little with docker container. We would dive deep into the basics of working with Docker containers, from launching and running containers to managing their lifecycle.

Running Containers
To run a container, you use the 'docker run* command followed by the name of the image you want to use. 
Recall that we pulled an ubuntu image from the official ubuntu repository on docker hub. Let's create a container from the ubuntu image. This command launches a container based on the Ubuntu image,

docker run ubuntu

To start running a container, we use:

docker start CONTAINER_ID

Launching Containers with Different Options
Docker provides various options to customize the behavior of containers. For example, you can specify environment variables, map ports, and mount volumes. Here's an example of running a container with a specific environment variable:

docker run -e "MY_VARIABLE=my-value" ubuntu

Running Containers in the Background

By default, containers run in the foreground, and the terminal is attached to the container's standard input/output. To run a container in the background, use the -d' option:

docker run -d ubuntu
This command starts a container in the background, allowing you to continue using the terminal.

Container Lifecycle

Containers have a lifecycle that includes creating, starting, stopping, and restarting. Once a container is created, it can be started and stopped multiple times. Starting, Stopping, and Restarting Containers.

To start a stopped container:
docker start container name

To stop a running container:
docker stop container name

To restart a container:
docker restart container name

Removing Containers
To remove a container, you use the docker rm' command followed by the container's ID or name:
docker rm container name

This deletes the container, but keep in mind that the associated image remains on your system. In this module, you've learned the basics of working with Docker containers—launching them, customizing their behavior, managing their lifecycle, and removing them.

Understanding these fundamentals is crucial for effectively using Docker in your development and deployment workflows.


Mini Project-Setting Up Minikube

Container Orchestration With Kubernetes:
Imagine orchestrating a vibrant culinary event with various chefs preparing different dishes. Container orchestration, seamlessly coordinates each chef (container) to ensure the perfect serving, timing, and overall dining experience. Just as a skilled event planner brings order to chaos, Kubernetes, a notable orchestrator, orchestrates containers, making it the go-to choice for managing the intricate dance of modern applications. 

Container orchestration is a critical aspect of managing and scaling containerized applications efficiently. It involves automating the deployment, scaling, and operation of containerized applications, ensuring seamless coordination among multiple containers to deliver high availability and optimal performance. One widely used container orchestration tool is Kubernetes. Developed by Google, Kubernetes has become the de facto standard for automating the deployment, scaling, and management of containerized applications.

What is Kubernetes ?
Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. Developed by Google and later open-sourced. Kubernetes is widely adopted for its ability to streamline and automate the deployment, scaling, and management of containerized applications in a highly efficient and consistent manner. It provides a centralized platform that abstracts away the complexities of distributed systems, offering features such as automated load balancing, self-healing capabilities, and seamless rolling updates.

Components Of A Kubernetes Cluster

Control Plane or Master Node: The Control Plane, often referred to as the master node, is the brain of a Kubernetes cluster. It manages the entire cluster, making high-level decisions about the state of the system. Components like the API Server, Scheduler, Controller Manager, and etcd (key-value store for cluster data) constitute the Control Plane.

Nodes: Nodes are individual machines within a Kubernetes cluster responsible for running containerized applications. Each node is equipped with a container runtime (e.g., Docker), a kubelet (communicates with the master and manages containers on the node), and a kube-proxy (maintains network rules). Nodes execute and manage containers, distribute workloads, and communicate with the control plane to maintain the desired state of the system. The collaboration of multiple nodes creates a scalable and resilient environment, forming the foundation for deploying and orchestrating containerized applications in Kubernetes.

Pods: Pods are the fundamental deployment units in Kubernetes, representing one or more closely related containers that share the same network namespace, storage, and set of specifications. Containers within a Pod work together and are scheduled to run on the same node. Pods encapsulate the basic building blocks for deploying and scaling applications, fostering tight communication between co located containers.

Containers: Containers in Kubernetes encapsulate and package applications, along with their dependencies and runtime environment, ensuring consistency across various computing environments. Leveraging containerization technology, such as Docker, containers provide a lightweight, portable, and efficient way to deploy and run applications. In Kubernetes, containers are organized into Pods, the smallest deployable units.

API Server: The API Server is the control plane component in Kubernetes that serves as the front end for the Kubernetes control plane. It exposes the Kubernetes API, allowing users, other components, and external entities to interact with the cluster, managing resources, and initiating various actions.

Controller Manager: The Controller Manager is a control plane component in Kubernetes responsible for maintaining the desired state of the cluster. It includes various controllers that watch the state of the cluster through the API Server and take corrective actions to ensure that the actual state aligns with the desired state.

Scheduler: The Scheduler is a control plane component in Kubernetes that assigns workloads to nodes in the cluster based on resource requirements, constraints, and availability. It plays a crucial role in distributing workloads evenly and efficiently across the worker nodes, optimizing resource utilization.

etcd: etcd is a distributed key-value store that acts as the cluster single source of truth for all persistent cluster data. In Kubernetes, etcd is used to store configuration details, state information, and metadata, providing a reliable and consistent data store that ensures the integrity of the cluster's information.

Kubelet: The Kubelet is a vital component in Kubernetes running on each node in the cluster. It communicates with the Kubernetes control plane specifically the API Server, to ensure that containers are running in a Pod as expected. Kubelet plays a key role in managing the containers on a node, handling tasks such as starting, stopping, and monitoring container processes based on the specifications received from the control plane.

Kube Proxy: Kube Proxy, or Kubernetes Proxy, is responsible for maintaining network rules on nodes. It enables communication between Pods and external entities, handling network routing and ensuring that the correct network policies are applied.

Minikube
Now that we have an idea of what kubernetes is, let's set up a minikube. But what is minikube ?

Minikube is an open-source tool that enables us to run Kubernetes clusters locally our machines. As we now know that kubernetes is a container orchestration platform that automates the deployment, scaling, and management of containerized applications. Minikube streamlines the creation of a local Kubernetes environment, providing a user-friendly playground where we can safely build and test their applications before deploying them to a production setting.
Let's get started with setting up minikube.

Installing Minikube on Linux
For linux users, let's install minikube
1. Launch a terminal with administrative access
2. We need to install docker as a driver for minikube and also for minikube to pull base images for the kubernetes cluster
3. Install minikube and start
	curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
	sudo dpkg -i minikube_latest_amd64.deb

	minikube start --driver=docker 

4. Kubectl is the command-line interface (CLI) tool for interacting with and managing Kubernetes clusters, allowing users to deploy, inspect, and manage applications within the Kubernetes environment. 

Let's install kubectl
sudo snap install kubectl --classic

This will download the kubernetes command line (kubectl) tool to interact with kubernetes cluster

Mini Project-Working with Kubernetes Node
Kubernetes Nodes

Now that we have our minikube cluster setup, let's dive into nodes in kubernetes

What Is a Node
In Kubernetes, think of a node as a dedicated worker, like a dependable employee in an office, responsible for executing tasks and hosting containers to ensure seamless application performance. A Kubernetes Node is a physical or virtual machine that runs the Kubernetes software and serves as a worker machine in the cluster Nodes are responsible for running Pods, which are the basic deployable units in Kubernetes. Each node in a kubernetes cluster typically represents a single host system.

Managing Nodes in kubernetes:
Minikube simplifies the management of Kubernetes for development and testing purposes. But in the context of minikube (a kubernetes cluster), we need to start it up before we can be able to access our cluster.

Start Minikube Cluster:
minikube start

This command starts a local Kubernetes cluster (minikube) using a single-node Minikube setup. It provisions a virtual machine (VM) as the Kubernetes node.

Stop Minikube Cluster:
minikube stop

Stops the running Minikube (local kubernetes cluster), preserving the cluster state,

Delete Minikube Cluster:
minikube delete

Deletes the Minikube kubernetes cluster and its associated resources.

View Nodes:
kubectl get nodes

Inspect a Node:
kubectl describe node <node-name>

Provides detailed information about a specific node, including its capacity, allocated resources, and status.


Node Scaling and Maintenance:
Minikube, as it's often used for local development and testing, scaling nodes may not be as critical as in production environments. However, understanding the concepts is beneficial:

Node Scaling: Minikube is typically a single-node cluster, meaning you have one worker node. For larger, production-like environments.
Node Upgrades: Minikube allows you to easily upgrade your local cluster to a new Kubernetes version, ensuring that your development environment aligns with the target production version,

By effectively managing nodes in Minikube kubernetes cluster, we can create, test, and deploy applications locally, simulating a kubernetes cluster without the need for a full-scale production setup. This is particularly useful for debugging, experimenting, and developing applications in a controlled environment.


Pods in Kubernetes
Definition and Purpose:

A Pod in Kubernetes is like a small container for running parts of an application. It can have one or more containers inside it that work closely together. These containers share the same network and storage, which makes them communicate and cooperate easily. A Pod is the smallest thing you can create and manage in Kubernetes. In Minikube, which is a tool to run Kubernetes easily, Pods are used to set up, change the size, and control applications.

Creating and Managing Pods:
Interaction with Pods in Minikube involves using the powerful kubectl* command-line tool. kubectl& is the command-line interface (CLI) tool for interacting with Kubernetes clusters. It allows users to deploy and manage applications, inspect and manage cluster resources, and execute various commands against Kubernetes clusters.

1. List Pods:
	kubectl get po -A
	This command provides an overview of the current status of Pods within the Minikube cluster.

2. Inspect a Pod:
	kubectl describe pod <pod-name>
	The command above can be used to gain detailed insights into a specific Pod, including events, container information, and overall configuration.

3. Delete a Pod:
	kubectl delete pod <pod-name>
	Removing a Pod from the Minikube cluster is as simple as issuing this command.

Containers in Kubernetes
Definition and Purpose:
From our knowledge of docker, we know Container represents a lightweight, standalone, and executable software package that encapsulates everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Containers are the fundamental units deployed within Pods, which are orchestrated by Kubernetes. In Minikube, containers play a central role in providing a consistent and portable environment for applications, ensuring they run reliably across various stages of the development lifecycle.

Integrating Containers into Pods:
Pod Definition with Containers: In the Kubernetes world, containers come to life within Pods. Developers define a Pod YAML file that specifies the containers to run, their images, and other configuration details. This Pod becomes the unit of deployment, representing a cohesive application. Using kubectl, we can deploy Pods and, consequently, the containers within them to the Minikube cluster. This process ensures that the defined containers work in concert within the shared context of a Pod.


Mini Project-Working with Kubernetes Resources
Share Project

Introduction To YAML
A Kubernetes YAML file is a text file written in YAML syntax that describes and defines Kubernetes resources. YAML is a human-readable data serialization format that is commonly used for configuration files. In the context of Kubernetes, these YAML files serve as a declarative way to specify the desired state of the resources such as pods, container, service and deployment you want to deploy and manage within a Kubernetes cluster.

Basic Structure Of YAML File
YAML uses indentation to represent the hierarchy of data, and it uses whitespace (usually spaces, not tabs) for indentation.

key1: value1
key2:
  subkey1: subvalue1
  subkey2: subvalue2
key3:
  - item1
  - item2


Data Types
Scalars Scalars are single values.

Strings:
name: John Doe

Numbers:
age: 25

Booleans:
is student:

Collections
Lists (arrays):
fruits:
  - apple
  - banana
  - orange

Maps (key-value pairs):
person :
	name: Alice
	age: 30

Nested Structures YAML allows nesting of structures:
employee:
	name: John Doe
	position: developer
	skills:
	  - Python
	  - JavaScript

Comments
In YAMLI comments starts with `#`
# This is a comment
key: value

Multiline Strings
Multiline strings can be represented using the description `|` or `>`

description: |
	This is a multiline
	string in YAML.

Anchors and Aliases 
You can use `&` to create an anchor and `*` to create an alias:
first: &name John
second: *name

In this example, second' has the same value as first

Now that you've got the basics, practice writing and reading YAML to become comfortable with its syntax. It's widely used in configuration files for various tools and systems.

Deploying Applications in Kubernetes
In Kubernetes, deploying applications is a fundamental skill that every beginner needs to grasp. Deployment involves the process of taking your application code and running it on a Kubernetes cluster, ensuring that it scales, manages resources efficiently, and stays resilient. This hands-on project will guide you through deploying your first application using Minikube, a lightweight, single-node Kubernetes cluster perfect for beginners.

Deployments in Kubernetes:
In Kubernetes, a Deployment is a declarative approach to managing and scaling applications. It provides a blueprint for the desired state of your application, allowing Kubernetes to handle the complexities of deploying and managing replicas. Whether you're running a simple web server or a more complex microservices architecture, Deployments are the cornerstone for maintaining application consistency and availability.

Services in Kubernetes:
Once your application is deployed, it needs a way to be accessed by other parts of your system or external users. This is where Services come into play. In Kubernetes, a Service is an
abstraction that defines a logical set of Pods and a policy by which to access them. It acts as a stable endpoint to connect to your application, allowing for easy communication within the cluster or from external sources. Some of the several types of Services in Kubernetes;

- ClusterlP: Purpose: The default type. Exposes the Service on a cluster-internal IP. Accessible only within the cluster.

- NodePort: Exposes the Service on each Node's IP at a static port (NodePort). Accessible externally using `NodeIP`.

- LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. Accessible externally through the load balancer's IP.

In subsequent sections, we will dive deep into deployment strategies and service configurations within the Kubernetes ecosystem, delving into the intricacies of these components to ensure a thorough understanding and proficiency in their utilization.

Deploying a Minikube Sample Application I-Ising YAML files for deployments and services in Kubernetes is like crafting a detailed plan for your application, while direct deployment with kubectl* commands is more like giving quick, on-the-spot instructions to launch and manage your application. 

Let's create a minikube deployment and service with kubectl'
	kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0
The command above creates a Kubernetes Deployment named "hello-minikube" running the container image

kubectl expose deployment hello-minikube --type=NodePort --port=8080
The command above exposes the Kubernetes Deployment named "hello-minikube" as a NodePort service on port 8080

kubectl get services hello-minikube
The easiest way to access this service is to let minikube launch a web browser for you.

Working With YAML Files
Let's recall our docker foundations project when we pushed an image we built to docker hub, Now let reuse our image in our yaml script for deployment.
i. Create a new folder my-nginx-yaml'
ii. Create a new file nginx-deployment .yaml* and paste the content below

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-nginx
  template:
    metadata:
      labels:
        app: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: dareyregistry/my-nginx:1.0
        ports:
        - containerPort: 80

The provided YAML snippet defines a Kubernetes Deployment for deploying an instance of the Nginx web server. Let's break down the key components:

apiVersion: apps/vl: Specifies the Kubernetes API version for the object being created, in this case, a Deployment in the "apps" group.

kind: Deployment: Defines the type of Kubernetes resource being created, which is a Deployment, Deployments are used to manage the deployment and scaling of applications.

metadata: Contains metadata for the Deployment, including the name of the Deployment, which is set to "my-nginx-deployment."

spec: Describes the desired state of the Deployment.

replicas: I: Specifies that the desired number of replicas (instances) of the Pods controlled by this Deployment is I.

selector: Defines how the Deployment selects which Pods to manage. In this case, it uses the label "app: my-nginx" to match Pods.

template: Specifies the template for creating new Pods.

metadata: Contains labels for the Pods, and in this caser the label is set to Wapp: my-nginx."

spec: Describes the Pod specification.

containers: Defines the containers within the Pod.

name: my-nginx: Sets the name of the container to "my-nginx."

image: dareyregistry/my-nginx:1.O: Specifies the Docker image to be used for the Nginx container. The image is "ridwanaz/my-nginx" with version "1.0.".
Note you can replace the image with your own image

ports: Specifies the port mapping for the container, and in this case, it exposes port 80.

iii. Create a new file called nginx-service .yaml* and paste the content below

apiVersion: v1
kind: Service
metadata:
  name: my-nginx-service
spec:
  selector:
    app: my-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort



The provided YAML snippet defines a Kubernetes Service for exposing the Nginx application to the external world. Let's break down the key components:

apiVersion: VI: Specifies the Kubernetes API version for the object being created, in this case, a Service.

kind: Service: Defines the type of Kubernetes resource being created, which is a Service. Services provide a stable endpoint for accessing a set of Pods.

metadata: Contains metadata for the Service, including the name of the Service, which is set to "my-nginx-service."

spec: Describes the desired state of the Service.

selector: Specifies the labels used to select which Pods the Service will route traffic to. In this case, it selects Pods with the label "app: my-nginx."

ports: Specifies the ports configuration for the Service.

protocol: TCP: Specifies the transport layer protocol, which is TCP in this case.

port: 80: Defines the port on which the Service will be exposed.

targetPort: 80: Specifies the port on the Pods to which the traffic will be forwarded.

type: NodePort: Sets the type of the Service to NodePort. This means that the Service will be accessible externally on each Node's IP address at a static port, which will be automatically assigned unless specified.
iv. Run the command below for the deployment on the cluster

	kubectl apply -f nginx-deployment.yaml

	kubectl apply -f nginx-service.yaml

v. Verify your deployment

	kubectl get deployments

	kubectl get services

vi. Access your deployment on web server
	minikube service my-nginx-service --url


Mini Project - Networking in Kubernetes
Networking In Kubernetes

Networking refers to the mechanisms and configurations that allow communication between different components (pods, services, and other resources) within a Kubernetes cluster.

Kubernetes provides a flexible and powerful networking model to enable seamless interaction between containers and services, whether they are running on the same node or across different nodes in a cluster.

Some key aspects of networking in Kubernetes
Pod Networking: Containers within a pod share the same network namespace, allowing them to communicate with each other using localhost, This enables tight coupling between containers within the same pod.

Service Networking: Kubernetes Services provide a way to expose a group of pods as a single, stable network endpoint. Services have an associated Cluster IP that allows other pods to communicate with the service. Services can be exposed internally within the cluster or externally to the outside world.

Pod-to-Pod Communication: Pods communicate with each other using their individual IP addresses. Kubernetes ensures that pods can reach each other directly regardless of the node they are running on, by using an overlay network.

Ingress: Ingress is a Kubernetes resource that allows external access to services within the cluster. It defines rules for routing external HTTP and HTTPS traffic to different services based on the host or path. Ingress controllers manage the actual routing and traffic flow.

Network Policies: Kubernetes Network Policies define rules for controlling the communication between pods. These policies allow administrators to specify how pods can communicate with each other, enhancing security within the cluster.

Container Network Interface (CNI): Kubernetes relies on Container Network Interfaces to implement networking solutions. CNIs provide a standardized interface for networking plugins to integrate with Kubernetes, allowing for flexibility and choice in networking implementations.

Let's get our hands on pod networking in Kubernetes by deploying a pod with multiple containers, showcasing how they share the same network namespace and can communicate with each other using localhost*. Here's a step-by-step guide using Kubernetes and kubectl'

1. Create a Mufti-Container Pod YAML file (e.g., 'multi -container-pod.yamL and paste the snippet below

apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: container-1
    image: nginx:alpine
  - name: container-2
    image: busybox
    command:
      - '/bin/sh'
      - '-c'
      - 'mkdir -p /usr/share/nginx/html && while true; do echo "Hello from Container 2" >> /usr/share/nginx/html/index.html; sleep 10; done'

Explanation of the yaml snippet above:

apiVersion: VI: Specifies the Kubernetes API version for the object being created, in this case, a Pod.

kind: Pod: Defines the type of Kubernetes resource being created, which is a Pod. Pods are the smallest deployable units in Kubernetes and can host one or more containers.

metadata: Contains metadata for the Pod, including the name of the Pod, which is set to "multi-container-pod."

spec: Describes the desired state of the Pod.

containers: Specifies the containers configuration for the Pod.
name: container-I: Defines the first container in the Pod with the name "container-I" and uses the nginx: alpine* image.
name: container-2: Defines the second container in the Pod with the name "container-2" and uses the busybox& image. Additionally, it specifies a command to create an HTML file in the Nginx directory and continuously appends "Hello from Container 2" to it every 10 seconds.

The pod has two containers - one running the Nginx web server and another running BusyBox with a simple command to continuously append "Hello from Container 2" to the Nginx default HTML file.

2. Apply the Pod Configuration:
	kubectl apply -f multi-container-pod.yaml

3. CHeck Pod Status and Log
	kubectl get pods
	kubectl logs multi-container-pod -c container-1
	kubectl logs multi-container-pod -c container-2

You'll observe that both containers are running within the same pod, and they share the same network namespace. The Nginx container serves its default page, and the BusyBox container continuously updates the HTML file.

4. Access NGINX from Busybox container
	kubectl exec -it multi-container-pod -c container-2 -- /bin/sh

Now, within the BusyBox container, you can use tools like curl or wget to access http://localhost and see the Nginx page.
This demonstrates how containers within the same pod can communicate with each other using localhost'


How To Install Kind (Kubernetes In Docker) and Create a KIND Cluster:

Install kind:
Download the kind binary and place it in your system's PATH. Here are the steps:

	curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64
	chmod +x ./kind
	sudo mv ./kind /usr/local/bin/kind

Verify the installation:
Check that kind is installed correctly by running:

	kind --version

Create a kind cluster:
Now, you can create a Kubernetes cluster using kind.

	kind create cluster
	
This command will create a default Kubernetes cluster. If you want a custom configuration, you 