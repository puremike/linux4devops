Notes on Linux:
# LINUX COMMANDS:

- tail: read the content of a file frm the last bottom. i.e tail -3 txt - reads the last 3 lines
- head: read the content of a file from the upper top. i.e head -3 txt - reads the first 3 lines 
- which: reveals the location of a command is stored (locate a command)
- info: reveals the information of a command
- cd: to change directory; cp or cp ~ - to the home directory; cp / - to the root directory
- mkdir: to create a new directory
- man:  reveal information of a command. Quite similar to the info command
- pwd: this command prints the working directory
- rm: remove a directory. using "rm --help or man rm" will bring several flags for the command
- cp: copy from a directory to another
- mv: mv a file from a directory to another; it also renames a file
- echo: output a content
- ls: a command for listing contents of a directory. using ls -r - reverse the order (sorting) of the content; using ls *.txt - will list every files ending with .txt extension; ls -d */ - will list only directories
- you can use the Ctrl + R + "search term" to find quickly find previously used command
- df: The df command stands for "disk-free" and shows available and used disk space on the linux system. df -h shows the disk space info in human readable format.
- free: the free shows the available  memory ram information on the system. free -h also shows the info in human readable form.

#TREE STRUCTURE ON LINUX

- mkdir with flag (-p) will create several nested directories.
	using tree command (after installing) will show the tree structure of a directory
	
	"sudo apt install tree" - will install tree
	
	tree -a (list hidden files in the tree)
	tree -d (list only directories in the tree)
	......'
- bin => usr/bin - directory referring to binary files in the linux
- lib => usr/lib / usr/lib32/ usr/lib64/ usr/libx32 - are referred to as libraries that help to runs specific programs.
- lost+found: It's a special directory that contains data that has become obsolete. Can be used to recover lost files.
- opt: a directory showing your custom application. Example, you'll find a VBox application in this folder
- proc: The /proc directory is a virtual filesystem that provides information about the system's processes and kernel.
- root: The root directory (/) is the mother of all files and directories of the Linux system. 
- boot: The boot directory hosts all the boot files for your linux operating system. It's  a don'touch directory except you know what you're doing.
- dev: Your device directory
- etc: The directory holding configuration files
- run: The run directory is the mount point for tempfs filesystem in the computer memory. Temporary data used by memory are kept there.
- sbin: Sbin directory stores the binaries required by the operating system for system management.
- snap: The /snap directory is, by default, where the files and folders from installed snap packages appear on your system. i.e. snap install tree
- swapfile: the directory holds: swap space in Linux is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space.
- srv: The term srv stands for service. The /srv directory contains site-specific data for your Linux distribution.
- var: /var is a standard directory that stands for "variable files.
- sys: /sys/class` is a directory in the Linux filesystem that provides a way to interact with the kernel and access information about various classes of devices and subsystems.
- usr: A very important directory that comprises libraries, binaries, and documentation for installed software applications. 

# OTHER COMMANDS TO USE IN THE LINUX FILE SYSTEM
- wget/curl: Wget is a command used to download files. It retrieves files using HTTP, HTTPS, and FTP protocols and is useful for downloads in unstable networks. use curl url --output "filename.ext"
- diff: diff command shows the difference between two files.
- vim/nano: the command helps to open and edit files using vim / nano respectively
- useradd: A command used to add a user 
- adduser: Used to create a new user. It also prompts you to enter password and other details for the new user immediately, unlike the useradd command.
- passwd: A command used to set password to a user
- userdel / deluser: Delete a user account and related files. sudo deluser --remove-all-files user_name 
- grep: Grep command is used to return. i.e. cat mytext.txt | grep es - will return contents with es; ls grep 01 will return files or directory with 01
- history: Bring back all the commands you've been using. You can also use a reverse search by typing Ctrl + R


# LINUX INODE (/df -i | /ls -i | /stat | /chmod | /In)
- stat: running: stat filename will reveal the metadata of the file


# LINUX - FILE PERMISSIONS

File Permissions and Access Rights
Understanding how to manage file permissions and ownership is crucial in Linux. This knowledge empowers you to control access to files and directories, ensuring the security and
integrity of your systenm Let's explore some essential commands and concepts related to file permissions and ownership.
In Linux, managing file permissions and ownership is vital for controlling who can access, modify, or execute files and directories. Understanding these concepts allows you to maintain
the security and integrity of your system, Let's delve into the key commands and concepts related to file permissions and ownership.
Numeric Representation of Permissions
In Linux, permissions are represented using numeric values. Each permission (no permission, read, write, and execute) is assigned a numeric value:
no permissions = O
read = 4
write = 2, and

These values are combined to represent the permissions for each user class. Lets consider a few examples.
Permissions Represented by 7
(read) + 2 (write) + 1 (execute) = 7
Symbolic: rwx
Meaning: Read, write, and execute permissions are all granted.
Example Context: A script file that the owner needs to read, modify, and execute.
Permissions Represented by 5
(read) + 1 (execute) = 5
Symbolic: r-x
Meaning: Read and execute permissions are granted, but write permission is not.
Example Context: A shared library or a command tool that users can execute and read but not modify.
Permissions Represented by 6
(read) + 2 (write) = 6
Symbolic: rw-
Meaning: Read and write permissions are granted, but execute permission is not.
Example Context: A document or a configuration file that the owner needs to read and modify but not execute.


Shorthand Representation of Permissions* *
In addition to the numeric way of showing permissions, Linux also has a shorthand, or symbolic, method for representing file permissions.
Understanding User Classes from a Permissions Perspective
Before diving into shorthand permissions, it's important to understand the concept of "user classes" in the context of Linux permissions. Think of user classes as categories of users that
Linux recognizes when deciding who can do what with a file. There are three main classes:
Owner: The person who created the file. Often referred to as 'user'.
Group: A collection of users who share certain permissions for the file.
Others: Anyone else who has access to the computer but doesn't fall into the first two categories.
The Role of Hyphens (-) in Permission Representation
When discussing permissions, you might notice hyphens (-) being mentioned. In the context of Linux file permissions, a hyphen doesn't actually represent a user class. Instead, it's
used in the symbolic representation of permissions to show the absence of a permission.
Lets get a bit practical with examples. Get onto your Linux terminal and run Is -latr

❯ ls -latr
total 24
-rw-r--r--   1 dare  staff  6332 Jan 29 22:44 README.md
drwxr-xr-x   7 dare  staff   224 Feb  4 11:56 Hands-On-Projects
drwxr-xr-x   8 dare  staff   256 Feb  4 11:56 .
-rwxr-xr-x   1 dare  staff   133 Feb  4 11:56 sync_img_to_s3.sh
drwxr-xr-x  37 dare  staff  1184 Feb  4 12:52 image
drwxr-xr-x   8 dare  staff   256 Feb  4 13:52 Quizzes
drwxr-xr-x   5 dare  staff   160 Feb  5 09:28 ..
drwxr-xr-x  15 dare  staff   480 Feb  6 21:34 .git


Let's break it down to understand what each part means:
In the output above, you will notice that some of the first character can be a - or d d means it's a directory, - means it's a file.
The next three characters (rwx) show the permissions for the owner. r stands for read, w for write, and x for execute.
If a permission is not granted, you'll see a - in its place (e.g., r-x means read and execute permissions are granted, but write permission is not).

The hyphen separates, owner, group, and others
The following three characters after the owner's permissions represent the group's permissions, using the same r, w, and x notation.
The last three characters show the permissions for others.
The order the user class is represented is as follow;
The first hyphen " "
- is the user
The second hyphen "-" is the group
The third hyphen "-" is others


# LINUX TERMINAL FILE DESCRIPTORS AND REDIRECTIONS
A file descriptor (FD, less frequently fildes) is a process-unique identifier (handle) for a file or other input/output resource, such as a pipe or network socket. Here are the three file descriptors:
	1. Standard In		stdin	0	Input from the keyboard
	2. Standard Out		stdout	1	Output to the console
	3. Standard Error	stderr	2	Error output to the console
	
Redirections: Redirection allows commands' file handles to be duplicated, opened, closed, made to refer to different files, and can change the files the command reads from and writes to. Redirection may also be used to modify file handles in the current shell execution environment.
	"find ./ -name newfile 2> stderr.txt" - It will redirect the error (stderr (2) - one of the 3 file descriptor) into the stderr.txt file and print to console the file path of the newfile.

- ps: The Process Status (ps) command in Linux is a powerful tool that allows you to view information about the processes running on your Linux system. We can also use the top or htop (after installing) to check all the processes running on the linux system.
	The ps -p <PID> command is pretty straightforward to get the process information of a PID. 
	The pwdx <PID> command will reveal the process file path
	
- lsof: lsof command stands for "List Open Files". This command provides a list of files that are opened. Basically, it gives the information to find out the files which are opened by which process. 	


# LINUX TERMINAL - USER GROUPS AND PERMISSIONS
- groupadd: command let you create a group which you can add any user into
	groupadd "group_name"
- groupdel: command to delete a groupd
	groupdel "group_name"
	
- usermod: the command let you modify the details of a user. I.e, 
	usermod -aG "group_name" "user" - will add the user to the group without removing the user from any existing group (because of the -G flag)



Some QUESTIONS/ANSWERS:

What command installs software on a Red Hat/Fedora-based system?
Ans => dnf install
 
What command is used to update the package lists on a Debian-based Linux system?
Ans => apt update 

How would you uninstall a software package on a Red Hat/Fedora-based system using the package manager?
Ans => yum remove package-name
 
Which command is used to clean the package cache on a Debian/Ubuntu system?
Ans => clean apt-cache

To upgrade all installed packages on a Debian-based system, which command would you use?
Ans => apt-get dist-upgrade


To list files in a directory, which command would you use?
Ans => ls

How do you navigate to the home directory of the current user?
Ans => cd ~ or cd

Which command is used to remove a file in Linux?
Ans => rm

What command is used to create a new directory?
Ans => mkdir

How do you display the current working directory in the Linux terminal?
Ans => pwd


To remove read and write permissions for the group and others on a file, what command would you use?
Ans => chmod go-rw file
	g - group
	o - others

What command is used to change the ownership of a file in Linux?
Ans => chown

How would you give read, write, and execute permissions to the owner of a file?
chmod u+rwx file
	owner is the user of the file = u

What command is used to set the sticky bit on a directory?
Ans => chmod 1777

Which command is used to change the group ownership of a file?
Ans =>chgrp


# ADVANCED FILE OPERATIONS
ARCHIVING AND COMPRESSING OF FILES
POPULAR METHODS OF ARCHIVING AND COMPRESSING:
- ZIP: The fact that it is cross-platform accessible gives this method an advantage over others. You can access and open a zip file regardless of the operating system because Linux, Windows®, and MacOS® all support zip files by default.
	$ zip -r <name of the zip file>.zip <directory or file(s) you want to compress>
	To archive a single, we can use the following command:
	$ zip example.zip examplefile.txt
	
	To add multiples or directory to the zip archive, we use:
	$ zip -r example.zip /home/users/Pictures - -r is to add all the files in the directory recursively
	
	If you received a zipped file, use the unzip command to unzip it:
	$ unzip examples.zip
	
	What if you want to unzip the file into your Pictures directory, and you are currently in a different directory?
	$ unzip -r examples.zip -d /home/user/Pictures
	Otherwise, the files will be extracted in the zipped directory

- TAR: The tar command has a few more options than the zip command did. The most commonly used options for the tar command
includes the following:

    -c: creates a new .tar archive file
    -v: verbosely shows the tar process so you can see all the steps in the process
    -f: specifies the file name type of the archive file
    -x: extracts files from an existing .tar file

The following example shows the basic syntax of the tar command to create an archive:
	$ tar -cvf examples.tar /home/user/Pictures
	$ tar -cvf examples.tar ‘*.jpg’ - will add all the .jpg files in the archive

To extract files from an archive in the same directory, use:
	$ tar -xvf examples.tar /home/user/Pictures

To extract files from an archive to a different directory, use:
	$ tar -xvf examples.tar -C /path/to/desired/directory/location/
	
- TAR.GZ: Tar.gz files add compression to the archive function of the tar command by using the gzip function.
	$ tar -zcvf <archive name>.tar.gz /directory/you/want/to/compress
	OR
	$ tar -zcvf <archive name>.tar.gz ‘*.jpg’
 


# SHELL SCRIPTING
- Shell Scripting: Is the process of writing and executing a series of instructions in a shell to automate tasks. A shell script is essentially a script or program written in a shell language, such as Bash, sh, zsh, or PowerShell.

- The bash Shell Program: The command-line interface you type into is a Bash Shell. It's specificallt a running instance of the program found at /bin/bash. Bash is an sh-compatible command language interpreter.

	A simple hello-world script:
	
	#! /bin/bash
	target="World"
	echo "Hello, ${target}"
	
	Bash Script can have comments. Using the hash (#) symbol before a code line will comment out the code. For example:
	
	# echo "Hello World!" - this line of code won't run because we've commented out the code using the # symbol.


# HANDS-ON: VERSION CONTROL SYSTEM AND WHY IT IS NEEDED

A Version Control System (VCS) is a vital tool in software development, designed to track and manage changes to code or documents over time. It enables multiple developers to collaborate on the same project efficiently, by controlling and merging changes made by different team members. 
Imagine you're working in a team and your project is about creating a website for a an Al startup company. The website includes various sections like Home, About Us, Services, and Contact Information, Each member of your team is responsible for a different section of the site. Without a Version Control System (VCS), managing this collaboration efficiently would be challenging.

Lets take a view of an example of working without a Version Control System. 

Overwriting Work
If a member "Tom" makes changes to the home page file "index.html" to update the navigation and at the same time, another team member "Jerry" makes changes to add contact information to the footer of the same home page thereby editing the same "index.html" file, Without VCSi the last person to upload their version of the file to the shared folder or server would overwrite the other person's changes, resulting in lost work.

How VCS Solves These Problems
Concurrent Development: With a VCS, each team member can work on their sections simultaneously without fear of overwriting each other's work. The VCS tracks all changes and manages different versions of the files, allowing changes to be merged together eventually. Lets go through an example together to simulate this experience using a VCS tool, "Git"

Introducing Git: A Leading Version Control System
Git is a tool that helps people work together on computer projects, like building a website. Think of it as a shared folder on your computer, but much smarter. It keeps track of all the changes everyone makes, so if something goes wrong, you can always go back to a version that worked. It also lets everyone work on their parts at the same time without getting in each other's way.
When working on a project, especially with a team, it's easy for things to get mixed up if you're not careful. For example, if two people try to change the same thing at the same time, it could cause problems. 

Git helps prevent these kinds of mix-ups.

Conceptualising Git Set Up with Tom and Jerry

1. Initial Setup:
Both Tom and Jerry have Git installed on their computers. 
They clone (or download) the project repository from a central repository (like GitHub, GitLab, or Bitbucket) to their local machines. This gives them each a complete copy of the project, including all its files and version history.

2. Tom and Jerry Start Working:
Tom and Jerry pull the latest changes from the central repository to ensure they start with the most current version of the index.html file. 
They both create a new branch from the main project. A branch in Git allows developers to work on a copy of the codebase without affecting the main line of development. Tom names his branch update-navigation, and Jerry names his add-contact-info.

3. Making Changes:
On his branch, Tom updates the navigation bar in index.html.
Simultaneously, Jerry works on his branch to add contact information to the footer of the same file.
They commit their changes to their respective branches. A commit in Git is like saving your work with a note about what you've done.

Merging Changes:
Once they're done, Tom and Jerry push their branches to the central repository. 
Tom decides to merge his changes first. He creates a pull request (PR) for his branch update-navigation. A PR is a way to tell the team that he's done and his code is ready to be reviewed and merged into the main project.
After reviewing Tom's changes, the team merges his PR into the main branch, updating the index.html file on the main project line.
Jerry then updates his branch with the latest changes from the main project to include Tom's updates. This step is crucial to ensure that Jerry is working with and integrating his changes into the most current version of the project.
Jerry resolves any conflicts that arise from Tom's changes and his own. Git provides tools and commands to help identify and resolve these conflicts.
Jerry then pushes his updated branch and creates a PR for his changes. The team reviews Jerry's additions, and once they're approved, his changes are merged into the main project.

# GIT BRANCHING AND MERGING

Part 3: Merging Changes 
After both Tom and Jerry have pushed their changes, you (or another team member) can review and merge these changes into the main project. The process involves
1. Creating a Pull Request
2. Merging the Pull Request into the main branch.

Understanding Pull Requests:
A Pull Request (PR) is a feature used in GitHub (and other Gitbased version control systems) that allows you to notify team members about the changes you've pushed to a branch in a repository. Essentially, it's a request to review and pull in your contribution to the main project. Pull requests are central to the collaborative development process, enabling team members to discuss, review, and make further changes before changes are merged.

How to Create a Pull Request on GitHub

After both Tom and Jerry have pushed their work to their respective branches, the next step is to create a pull request for each of them. Here's how Tom would create a pull request for his changes:
1. Navigate to Your GitHub Repository: Open your web browser and go to the GitHub page for the repository.
2. Switch to the Branch: Click on the branch dropdown menu near the top left corner of the file list and select the branch Tom have been working on, in this case, update-navigation branch.
3. Create New Pull Request: Click the "New pull request" button next to the branch dropdown menu.


GitHub will take you to a new page to initiate a pull request. It automatically selects the main project's branch as the base and your recently pushed branch as the compare branch.
4. Review Tom's Changes: Before creating the pull request, Tom would review his changes to ensure everything is correct. GitHub shows the differences between the base branch and Tom's branch. It's a good opportunity for Tom to double-check his work.

5. Create the Pull Request: If everything looks good, click the "Create pull request" button. Provide a title and description for the pull request. The title should be concise and descriptive, and the description should explain the change that the pull request is about, why it's needed, and any other relevant details.
After filling in the information, click 'Create pull request" again to officially open the pull request.

Reviewing and Merging Tom's Pull Request 
Once the pull request is created, it becomes visible to other team members who can review the changes, leave comments, and request additional modifications if necessary (This is an example of what collaboration is about in DevOps). When the team agrees that the changes are ready and good to go, someone with merge permissions can merge the pull request, incorporating the changes from Tom's update-navigation branch into the main branch.

Following the same process, Jerry would create a pull request for his add-contact-info branch after Tom's changes have been merged, ensuring that the project stays up to date and conflicts are minimized Updating Jerry's Branch with Latest Changes
Before Jerry merges his changes into the main branch, it's essential to ensure his branch is up-to-date with the main branch. This is because other changes (like Tom's updates) might have been merged into the
main branch after Jerry started working on his feature. Updating ensures compatibility and reduces the chances of conflicts.

Steps to Update Jerry's Branch:
On the terminal, Switch to Jerry's Branch:
	git checkout add-contact-info

Pull the Latest Changes from the Main Branch:
	git pull origin main

Purpose: This command fetches the changes from the main branch (Remember, main branch now has Tom's changes) and merges them into Jerry's add-contact-info branch. It ensures that any updates made to the main branch, like Tom's merged changes, are now included in Jerrys branch. This step is crucial for avoiding conflicts and ensuring that Jerry's work can smoothly integrate with the main project.
Merge the pull request to the main branch: Click the "Merge pull request" button to merge Tom's changes into the main branch. This action combines Tom's contributions with the rest of the project, completing the collaborative workflow.

Finalizing Jerry's Contribution
Assuming there are no conflicts, Jerry's branch is now ready to be merged back into the main project.

Push the Updated Branch to GitHub:
	git push origin add-contact-info
This command uploads Jerry's changes to GitHub. Now, his branch reflects both his work and the latest updates from the main branch.

The origin keyword in the command refers to the default name Git gives to the remote repository from which you cloned your project. It's like a shortcut or an alias for the full URL of the repository in GitHub.

Create the Pull Request (PR) for Jerry's changes, similar to how you did for Tom.
Merge Jerry's Pull Request. Complete the process by merging the PR into the main branch.

This simulated workflow illustrates how Git facilitates collaborative development, allowing multiple developers to work simultaneously on different aspects of a project and merge their contributions seamlessly, even when working on the same files.


Jamie's Work: Updating Events Page
Repeat the same flow for Jamie's work on Events Page. Ensure Jamie's work is in update-events branch.
Pull the latest changes from the main branch into update-events before raising the PR.


ENVIRONMENT VARIABLES 7 INFRASTRUCTURE ENVIRONMENTS:

Understanding Environment Variables & Infrastructure Environments: Key Differences
As we delve deeper into the world of technology and its building blocks, two essential concepts often come to the forefront: "Infrastructure Environments" and "Environment Variables."

Despite both terms featuring "Environment," they play distinct roles in the realm of scripting and software development. This common terminology can lead to confusion, making it crucial to distinguish and understand each concept from the outset. Infrastructure Environments Infrastructure environments refer to the various settings where software applications are developed. tested, and deployed, each serving a unique purpose in the software lifecycle. 

Lets say you are working with a development team to build a FinTech product. They have 2 different AWS accounts. The journey would be something like;
1. VirtualBox + Ubuntu: The development environment where all local development is done on your laptop 
2. AWS Account 1: The testing environment where, after local development is completed, the code is pushed to an EC2 instance here for further testing
3. AWS Account 2: The production environment, where after tests are completed in AWS Account 1:, the code is pushed to an EC2 instance in AWS Account 2, where the customers consume the Fintech product through a website.

On the other hand, environment variables are key-value pairs used in scripts or computer code to manage configuration values and control software behavior dynamically.

Environment Variables:
Imagine your FinTech product needs to connect to a database to fetch financial data. However, the details of this database connection, like the database URL, username, and password differ between your development, testing, and production environments. 
If you need to develop a shell script that will be reused across all the 3 different environments, then it is important to dynamically fetch the correct value for your connectivity to those environments.

Here's how environment variables come into play: 

Development Environment (VirtualBox + Ubuntu):
Environment Variables:
DB URL-localhost
DB USER=test user
DB_PASS=test_pass

Here, the environment variables point to a local database on your laptop where you can safely experiment without affecting real or test data.

Testing Environment (AWS Account 1):
Environment Variables:

DB_URL=testing-db.example.com
DB_USER=testing_user
DB_PASS=testing_pass

In this environment, the variables are configured to connect to a remote database dedicated to testing. This ensures that tests are performed in a controlled environment that simulates production settings without risking actual customer data.

Production Environment (AWS Account 2):
Environment Variables:

DB_URL=production-db.example.com
DB_USER=prod_user
DB_PASS=prod_pass

Finally, when the application is running in the production environment. The environment variables switch to ensure the application connects to the live database. This is where real customer interactions happen, and the data needs to be accurate and secure.
By clarifying these differences early on, we set a solid foundation for navigating the complexities of technology development with greater ease and precision. 

Now lets begin developing our shell script to manage cloud infrastructure.

aws_cloud_manager.sh script:
By the end of this mini project, we would have started working on the aws_cloud_manager.sh script where environment variables will be defined, and command line arguments are added to control if the script should run for local environment, testing or production environment.

Developing a shell script is usually done by starting with incremental changes. 
Lets begin by creating environment variable to determine if the script is running for local, testing, or production environment. 
If you are on Mac, open up your Mac Terminal 
If you are on Windows, login to your Ubuntu desktop in virtual box and open up the terminal.
If you don't use Virtual box, spin up an EC2 instance and name it local so it represents your local environment, then login to it.
Create a shell script with the name aws_cloud_manager.sh
Put the code below into the file:

#!/bin/bash

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
echo "Running script for Local Environment..."
# Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
echo "Running script for Testing Environment..."
# Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
echo "Running script for Production Environment..."
# Commands for production environment
else
echo "No environment specified or recognized."
exit 2
fi

Give it the regular permission to execute on your local machine:
sudo chmod +x aws_cloud_manager.sh

If you execute this as it is, the execution should go into the else block just because there is no $ENVIRONMENT variable set. 

what if you type this on your terminal:
export ENVIRONMENT=production

Then run the script again, the output this time will be:

Running script for Production Environment...

Now, you can see how environment variables can be used to dynamically apply logic in the script based on the requirement you are trying to satisfy.
The export command is used to set key and values for environment variables.

You can also set the variable directly within the script. For example if the script was like this:

#!/bin/bash

# Initialize environment variable
ENVIRONMENT="testing"

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
  # Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
  # Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
  # Commands for production environment
else
  echo "No environment specified or recognized."
  exit 2
fi

Running this version of the script would mean every time you run it. It will consider the logic for testing environment. Because the value has been "hard coded" in the script, and that is no longer dynamic.
The best way to do this would be to use command line arguments.

Positional parameters in Shell Scripting:
As we've learned, hard-coding values directly into scripts is considered poor practice. Instead, we aim for flexibility by allowing scripts to accept input dynamically, This is where positional parameters come in  a capability in shell scripting that enables passing arguments to scripts at runtime, and then replaces the argument with the parameter inside the script, the argument passed to the script is the value that is provided at runtime. 

As in the case of the below where the argument is "testing", and it is also the value to the variable within the script.

./aws_cloud_manager.sh testing

Inside the script, we'll have:

ENVIRONMENT=$1

`$1` is the positional parameter which will be replaced by the argument passed to the script. Because it is possible to pass multiple parameters to a script, dollar sign is used to prefix the position of the argument passed to the script. Imagine if another variable within the script is called NUMBER OF INSTANCES that determines how many EC2 instances get provisioned, then calling the script might look like;

./aws_cloud_manager.sh testing 5

The positional parameter inside the script will then look like:

ENVIRONMENT=$1
NUMBER_OF_INSTANCES=$2

Each positional parameter within the script corresponds to a specific argument passed to the script, and each parameter has a position represented by an index number.
In the case of:

./aws_cloud_manager.sh testing 5

We have two positional parameters, namely position 1( testing) and position 2 (5)


Creating shell scripts to meet specific requirements is one aspect of development, but ensuring their cleanliness and freedom from bugs is equally crucial. Integrating logical checks periodically to validate data is considered a best practice in script development.
A prime example of this is verifying the number of arguments passed to the script, ensuring that the script receives the correct input required for its execution, and providing clear guidance to users in case of incorrect usage.

Below code ensures that when the script is executed, exactly 1 argument is passed to it, otherwise it fails with an exit code of 1 and an shows a message telling the user how to use the script.

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

"$#" is a The special variable that holds the number of arguments passed to the script.
"-ne" means "Not equal"
"$0" represent the positional parameter of O, which is the script itself.
Hence, if number of arguments is not equal to then show the echo message.
An updated script would look like this;

#!/bin/bash

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

# Accessing the first argument
ENVIRONMENT=$1

# Acting based on the argument value
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
else
  echo "Invalid environment specified. Please use 'local', 'testing', or 'production'."
  exit 2
fi


ENVIRONMENT VARIABLES 7 INFRASTRUCTURE ENVIRONMENTS:

Understanding Environment Variables & Infrastructure Environments: Key Differences
As we delve deeper into the world of technology and its building blocks, two essential concepts often come to the forefront: "Infrastructure Environments" and "Environment Variables."

Despite both terms featuring "Environment," they play distinct roles in the realm of scripting and software development. This common terminology can lead to confusion, making it crucial to distinguish and understand each concept from the outset. Infrastructure Environments Infrastructure environments refer to the various settings where software applications are developed. tested, and deployed, each serving a unique purpose in the software lifecycle. 

Lets say you are working with a development team to build a FinTech product. They have 2 different AWS accounts. The journey would be something like;
1. VirtualBox + Ubuntu: The development environment where all local development is done on your laptop 
2. AWS Account 1: The testing environment where, after local development is completed, the code is pushed to an EC2 instance here for further testing
3. AWS Account 2: The production environment, where after tests are completed in AWS Account 1:, the code is pushed to an EC2 instance in AWS Account 2, where the customers consume the Fintech product through a website.

On the other hand, environment variables are key-value pairs used in scripts or computer code to manage configuration values and control software behavior dynamically.

Environment Variables:
Imagine your FinTech product needs to connect to a database to fetch financial data. However, the details of this database connection, like the database URL, username, and password differ between your development, testing, and production environments. 
If you need to develop a shell script that will be reused across all the 3 different environments, then it is important to dynamically fetch the correct value for your connectivity to those environments.

Here's how environment variables come into play: 

Development Environment (VirtualBox + Ubuntu):
Environment Variables:
DB URL-localhost
DB USER=test user
DB_PASS=test_pass

Here, the environment variables point to a local database on your laptop where you can safely experiment without affecting real or test data.

Testing Environment (AWS Account 1):
Environment Variables:

DB_URL=testing-db.example.com
DB_USER=testing_user
DB_PASS=testing_pass

In this environment, the variables are configured to connect to a remote database dedicated to testing. This ensures that tests are performed in a controlled environment that simulates production settings without risking actual customer data.

Production Environment (AWS Account 2):
Environment Variables:

DB_URL=production-db.example.com
DB_USER=prod_user
DB_PASS=prod_pass

Finally, when the application is running in the production environment. The environment variables switch to ensure the application connects to the live database. This is where real customer interactions happen, and the data needs to be accurate and secure.
By clarifying these differences early on, we set a solid foundation for navigating the complexities of technology development with greater ease and precision. 

Now lets begin developing our shell script to manage cloud infrastructure.

aws_cloud_manager.sh script:
By the end of this mini project, we would have started working on the aws_cloud_manager.sh script where environment variables will be defined, and command line arguments are added to control if the script should run for local environment, testing or production environment.

Developing a shell script is usually done by starting with incremental changes. 
Lets begin by creating environment variable to determine if the script is running for local, testing, or production environment. 
If you are on Mac, open up your Mac Terminal 
If you are on Windows, login to your Ubuntu desktop in virtual box and open up the terminal.
If you don't use Virtual box, spin up an EC2 instance and name it local so it represents your local environment, then login to it.
Create a shell script with the name aws_cloud_manager.sh
Put the code below into the file:

#!/bin/bash

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
echo "Running script for Local Environment..."
# Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
echo "Running script for Testing Environment..."
# Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
echo "Running script for Production Environment..."
# Commands for production environment
else
echo "No environment specified or recognized."
exit 2
fi

Give it the regular permission to execute on your local machine:
sudo chmod +x aws_cloud_manager.sh

If you execute this as it is, the execution should go into the else block just because there is no $ENVIRONMENT variable set. 

what if you type this on your terminal:
export ENVIRONMENT=production

Then run the script again, the output this time will be:

Running script for Production Environment...

Now, you can see how environment variables can be used to dynamically apply logic in the script based on the requirement you are trying to satisfy.
The export command is used to set key and values for environment variables.

You can also set the variable directly within the script. For example if the script was like this:

#!/bin/bash

# Initialize environment variable
ENVIRONMENT="testing"

# Checking and acting on the environment variable
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
  # Commands for local environment
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
  # Commands for testing environment
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
  # Commands for production environment
else
  echo "No environment specified or recognized."
  exit 2
fi

Running this version of the script would mean every time you run it. It will consider the logic for testing environment. Because the value has been "hard coded" in the script, and that is no longer dynamic.
The best way to do this would be to use command line arguments.

Positional parameters in Shell Scripting:
As we've learned, hard-coding values directly into scripts is considered poor practice. Instead, we aim for flexibility by allowing scripts to accept input dynamically, This is where positional parameters come in  a capability in shell scripting that enables passing arguments to scripts at runtime, and then replaces the argument with the parameter inside the script, the argument passed to the script is the value that is provided at runtime. 

As in the case of the below where the argument is "testing", and it is also the value to the variable within the script.

./aws_cloud_manager.sh testing

Inside the script, we'll have:

ENVIRONMENT=$1

`$1` is the positional parameter which will be replaced by the argument passed to the script. Because it is possible to pass multiple parameters to a script, dollar sign is used to prefix the position of the argument passed to the script. Imagine if another variable within the script is called NUMBER OF INSTANCES that determines how many EC2 instances get provisioned, then calling the script might look like;

./aws_cloud_manager.sh testing 5

The positional parameter inside the script will then look like:

ENVIRONMENT=$1
NUMBER_OF_INSTANCES=$2

Each positional parameter within the script corresponds to a specific argument passed to the script, and each parameter has a position represented by an index number.
In the case of:

./aws_cloud_manager.sh testing 5

We have two positional parameters, namely position 1( testing) and position 2 (5)


Creating shell scripts to meet specific requirements is one aspect of development, but ensuring their cleanliness and freedom from bugs is equally crucial. Integrating logical checks periodically to validate data is considered a best practice in script development.
A prime example of this is verifying the number of arguments passed to the script, ensuring that the script receives the correct input required for its execution, and providing clear guidance to users in case of incorrect usage.

Below code ensures that when the script is executed, exactly 1 argument is passed to it, otherwise it fails with an exit code of 1 and an shows a message telling the user how to use the script.

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

"$#" is a The special variable that holds the number of arguments passed to the script.
"-ne" means "Not equal"
"$0" represent the positional parameter of O, which is the script itself.
Hence, if number of arguments is not equal to then show the echo message.
An updated script would look like this;

#!/bin/bash

# Checking the number of arguments
if [ "$#" -ne 0 ]; then
    echo "Usage: $0 <environment>"
    exit 1
fi

# Accessing the first argument
ENVIRONMENT=$1

# Acting based on the argument value
if [ "$ENVIRONMENT" == "local" ]; then
  echo "Running script for Local Environment..."
elif [ "$ENVIRONMENT" == "testing" ]; then
  echo "Running script for Testing Environment..."
elif [ "$ENVIRONMENT" == "production" ]; then
  echo "Running script for Production Environment..."
else
  echo "Invalid environment specified. Please use 'local', 'testing', or 'production'."
  exit 2
fi



INTRODUCTION TO DOCKER AND CONTAINERS

What are Containers?

In realm of software development and deployment, professionals used to face a dilemma. They crafted brilliant code on their local machines, only to find that when deployed to other environments, it sometimes does not work. The culprit? The notorious "it works on my machine" phenomenon.

Get started with Docker, a tool that emerged to a major problem IT industry. A man named Solomon Hykes, who, in 2013, unveiled Docker, a containerisation platform that promised to revolutionize the way IT professionals built, shipped, and ran applications. Imagine containers as magical vessels that encapsulate everything an application needs to run smoothly — its code, libraries, dependencies, and even a dash of configuration magic. These containers ensure that an application remains consistent and behaves the same, whether it's running on a developer's laptop, a testing server, or a live production environment. Docker had bestowed upon IT professionals the power to say goodbye to the days of "it works on my machine."


Advantages of Containers 
Portability Across Different Environments: In the past, deploying applications was akin to navigating a treacherous maze, with compatibility issues lurking at every turn. Docker's containers, However, encapsulate the entire application along with its dependencies and configurations. This magical package ensures that your creation dances gracefully across different platforms, sparing you from the woes of the "it works on my machine" curse. With Docker, bid farewell to the headaches of environment mismatches and embrace a world where your application reigns supreme, irrespective of its hosting kingdom. 

Resource Efficiency Compared to Virtual Machines: Docker containers share the underlying host's operating system kernel, making them lightweight and nimble. This efficiency allows you to run multiple containers on a single host without the extravagant resource demands of traditional virtual machines. Picture Docker containers as magical carriages, swiftly transporting your applications without burdening the kingdom with unnecessary excess. With Docker, revel in the harmony of resource optimization and application efficiency. 

Rapid Application Deployment and Scaling: Docker containers can be effortlessly spun up or torn down, facilitating the swift deployment of applications. Whether you're facing a sudden surge in demand or orchestrating a grand-scale production, Docker allows you to scale your applications seamlessly. Imagine commanding an army of containers to conquer the peaks of user demand, only to gracefully retreat when the storm has passed. With Docker, the ability to scale becomes a wand in your hand, transforming the challenges of deployment into a choreographed ballet of efficiency. 


Comparison of Docker Container with Virtual Machines Docker and virtual machines (VMs) are both technologies used for application deployment, but they differ in their approach to virtualization. Virtual machines emulate entire operating systems, resulting in higher resource overhead and slower performance. In contrast, Docker utilizes containerization, encapsulating applications and their dependencies while sharing the host OSs kernel. This lightweight approach reduces resource consumption, provides faster startup times, and ensures portability across different environments. 

Docker's emphasis on microservices and standardized packaging fosters scalability and efficiency, making it a preferred choice for modern, agile application development, whereas virtual machines excel in scenarios requiring stronger isolation but at the cost of increased resource usage. The choice between Docker and VMS depends on specific use cases and the desired balance between performance and isolation.


Importance of Docker
Technology and Industry Impact: The significance of Docker in the technology landscape cannot be overstated, Docker and containerization have revolutionized software development, deployment, and management. The ability to package applications and their dependencies into lightweight, portable containers addresses key challenges in software development, such as consistency across different environments and efficient resource utilization. 

Real-World Impact: Implementing Docker brings tangible benefits to organizations. It streamlines the development process, promotes collaboration between development and operations teams, and  accelerates the delivery of applications. Docker's containerization technology enhances scalability, facilitates rapid deployment, and ensures the consistency of applications across diverse environments. 

This not only saves time and resources but also contributes to a more resilient and agile software development lifecycle.

Target Audience
This course on Docker is designed for a diverse audience, including: 

DevOps Professionals: Interested in container orchestration, seeking efficient ways to manage and deploy applications, improve resource utilization, and ensure system stability. 

Developers: Who want to streamline their application development, enhance collaboration, and ensure consistency across different stages of the development lifecycle. It caters to cloud engineers, OA engineers, and other tech enthusiast who are eager to enhance their technical skills and establish a strong foundation in docker and containersation.

Professionals seeking to expand their skill set or students preparing for roles in technology-related fields will find this project beneficial. 

Getting Started With Docker

Installing Docker

We need to launch an ubuntu 20.04 LTS instance and connect to it, then follow the steps below.

Before installing Docker Engine for the first time on a new host machine, it is necessary to configure the Docker repository. Following this setup, we can proceed to install and update 
Docker directly from the repository.

Now let's first add docker's oficial GPG key.
You can learn more about GPG keys

sudo apt-get update

This a Linux command that refreshes the package list on a Debian based system, ensuring the latest software information is available for installation.

sudo apt-get install ca-certificates curl gnupg

This a Linux command that installs essential packages including certificate authorities, a data transfer tool (curl), and the GNU Privacy Guard for secure communication and package verification.

sudo install -m e755 -d /etc/apt/keyrings

The command above creates a directory (/etc/apt/keyrings) with specific permissions (0755) for storing keyring files, which are used for docker's authentication 

curl -fsSL https://download.docker.com/linux/ubuntu/gpg I sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

This downloads the Docker GPG key using curl'

sudo chmod a+r /etc/apt/keyrings/docker.gpg

Sets read permissions for all users on the Docker GPG key file within the APT keyring directory

Let's add the repository to Apt sources

echo \ "deb --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg https://download.docker.com/linux/ubuntu \) stable" I \ $(. /etc/os-release && echo "$VERSION_CODENAME" sudo tee /etc/apt/sources.list.d/docker.list > /dev/null


Install latest version of docker

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

Verify that docker has been successfully installed

sudo systemctl status docker


By default, after installing docker, it can only be run by root user or using sudo* command. To run the docker command without sudo execute the command below

sudo usermod -aG docker ubuntu

After executing the command above, we can run docker command without using superuser privileges 

Running the "Hello World" Container
Using the 'docker runs Command

The docker run* command is the entry point to execute containers in Docker. It allows you to create and start a container based on a specified Docker image. The most straightforward example is the "Hello World" container, a minimalistic container that prints a greeting message when executed.

hello-world
# Run the "Hello World" container
docker run "Hello-World" container

When you execute this command, Docker performs the following steps: 

Pulls Image (if not available locally): Docker checks if the hello-world* image is available locally. If not, it automatically pulls it from the Docker Hub, a centralized repository for

1. Docker images.
2. Creates a Container: Docker creates a container based on the hello-world' image. This container is an instance of the image, with its own isolated filesystem and runtime environment.
3. Starts the Container: The container is started, and it executes the predefined command in the image, which prints a friendly message.

Understanding the Docker Image and Container Lifecycle

Docker Image: A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Images are immutable, meaning they cannot be modified once created, Changes result in the creation of a new image. 

Container Lifecycle: Containers are running instances of Docker images. They have a lifecycle: create, start, stop, and delete*. Once a container is created from an image, it can be started, stopped, and restarted.

Verifying the Successful Execution 
You can check if the images is now in your local environment with Example Output: 

docker images

Basic Docker Commands

Docker Run 
The docker run* command is fundamental for executing containers. It creates and starts a container based on a specified image.

# Run a container based on the "nginx" image 
docker run hello-world

This example pulls the "nginx" image from Docker Hub (if not available locally) and starts a container using that image.

Docker PS
The docker command displays a list of running containers. This is useful for monitoring active containers and obtaining information such as container IDs, names, and status.

# List running containers
docker ps

To view all containers, including those that have stopped, add the -a' option:

# List all containers (running and stopped)
docker ps -a

Docker Stop
The docker stop' command halts a running containel
# Stop a running container (replace CONTAINER ID with the actual container ID)
docker stop CONTAINER ID

Docker Pull 
The docker pull& command downloads a Docker image from a registry, such as Docker Hub, to your local machine.

# Pull the latest version of the "ubuntu" image from Docker Hub
docker pull ubuntu

Docker Push 
The docker push • command uploads a local Docker image to a registry, making it available for others to pull.

# Push a local image to Docker Hub
docker push your-username/image-name

Ensure you've logged in to Docker Hub using *docker before pushing images.

Docker Images
The docker images* command lists all locally available Docker images.
# List all local Docker images
docker images

Docker RMI 
The docker rmi* command removes one or more images from the local machine.
# Remove a Docker image (replace IMAGE_ID with the actual image ID)
docker rmi IMAGE ID

These basic Docker commands provide a foundation for working with containers. Understanding how to run, list, stop, pull, push, and manage Docker images is crucial for effective containerization and orchestration. As you delve deeper into Docker, you'll discover additional commands and features that enhance your ability to develop, deploy, and maintain containerized applications.
	

Working with Docker Images

Introduction to Docker Images
Docker images are the building blocks of containers. They are lightweight, portable, and self-sufficient packages that contain everything needed to run a software application, including the code, runtime, libraries, and system tools. Images are created from a set of instructions known as a Dockerfile, which specifies the environment and configuration for the application.

Pulling Images from Docker Hub 
Docker Hub is a cloud-based registry that hosts a vast collection of Docker images. You can pull images from Docker Hub to your local machine using the *docker pull* command. To explore available images on Docker Hub, the docker command provides a search subcommand. For instance, to find the Ubuntu image, you can execute: 

docker search ubuntu

This command allows you to discover and explore various images hosted on Docker Hub by providing relevant search results. In this case, the output will be similar to this: This command allows you to discover and explore various images hosted on Docker Hub by providing relevant search results. In this case, the output will be similar to this:

To download the official Ubuntu image to your computer, use the following command: 

docker pull ubuntu

Executing this command will fetch the official Ubuntu image from Docker Hub and store it locally on your machine, making it ready for use in creating containers.

Once an image has been successfully downloaded, you can proceed to run a container using that downloaded image by employing the "run" subcommand. Similar to the hello- world example, if an image is not present locally when the docker run* subcommand is invoked, Docker will automatically download the required image before initiating the container.

To view the list of images that have been downloaded and are available on your local machine, enter the following command: 

docker images

Executing this command provides a comprehensive list of all the images stored locally, allowing you to verify the presence of the downloaded image and gather information about its size, version, and other relevant details.

As we move on in this course, you will the images to work with containers

Dockerfile
A Dockerfile is a plaintext configuration file that contains a set of instructions for building a Docker image. It serves as a blueprint for creating a reproducible and consistent environment for your application. Dockerfiles are fundamental to the containerization process, allowing you to define the steps to assemble an image that encapsulates your application and its dependencies.

Creating a Dockerfile
In this Dockerfile file, we will be using an nginx image. *Nginx' is an open source software for web serving, reverse proxying, caching, load balancing, media streaming, and more. It started out as a web server designed for maximum performance and stability.

To create a Dockerfile, use a text editor of your choice, such as vim or nano. Start by specifying a base image, defining the working directory, copying files, installing dependencies, and configuring the runtime environment.

Here's a simple example of a Dockerfile for a html file: Let's create an image with using a Dockerfile. Paste the code snippet below in a file named Dockerfile. This example assumes you have a basic HTML file named index. html' in the same directory as your Dockerfile.

# Use the official NGINX base image
FROM nginx:latest

# Set the working directory in the container
WORKDIR  /usr/share/nginx/html/

# Copy the local HTML file to the NGINX default public directory
COPY index.html /usr/share/nginx/html/

# Expose port 80 to allow external access
EXPOSE 80

# No need for CMD as NGINX image comes with a default CMD to start the server

HTML file named "index.html" in the same directory as your dockerfile

To build an image from this Dockerfile, navigate to the directory containing the file and run: 

docker build -t dockerfile .

To run a container based on the custom NGINX image we created with a dockerfile, run the command:

docker run -p 8080:80 dockerfile

Running the command above will create a container that listens on port 8080 using the nginx image you created earlier. So you need to create a new rule in security group of the EC2 instance.

Let's recall the hands-on project we did in our advanced techops curriculum. Now let's add new rules to the security group:
	On our EC2 instance, click on the security tab,
	
	Click on edit inbound rules to add new rules. This will allow incoming traffic to instance associated with the security group. Our aim is to allow incoming traffic on port 8080


Introduction to Docker Containers

Docker containers are lightweight, portable, and executable units that encapsulate an application and its dependencies. In the previous step, we worked a little with docker container. We would dive deep into the basics of working with Docker containers, from launching and running containers to managing their lifecycle.

Running Containers
To run a container, you use the 'docker run* command followed by the name of the image you want to use. 
Recall that we pulled an ubuntu image from the official ubuntu repository on docker hub. Let's create a container from the ubuntu image. This command launches a container based on the Ubuntu image,

docker run ubuntu

To start running a container, we use:

docker start CONTAINER_ID

Launching Containers with Different Options
Docker provides various options to customize the behavior of containers. For example, you can specify environment variables, map ports, and mount volumes. Here's an example of running a container with a specific environment variable:

docker run -e "MY_VARIABLE=my-value" ubuntu

Running Containers in the Background

By default, containers run in the foreground, and the terminal is attached to the container's standard input/output. To run a container in the background, use the -d' option:

docker run -d ubuntu
This command starts a container in the background, allowing you to continue using the terminal.

Container Lifecycle

Containers have a lifecycle that includes creating, starting, stopping, and restarting. Once a container is created, it can be started and stopped multiple times. Starting, Stopping, and Restarting Containers.

To start a stopped container:
docker start container name

To stop a running container:
docker stop container name

To restart a container:
docker restart container name

Removing Containers
To remove a container, you use the docker rm' command followed by the container's ID or name:
docker rm container name

This deletes the container, but keep in mind that the associated image remains on your system. In this module, you've learned the basics of working with Docker containers—launching them, customizing their behavior, managing their lifecycle, and removing them.

Understanding these fundamentals is crucial for effectively using Docker in your development and deployment workflows.